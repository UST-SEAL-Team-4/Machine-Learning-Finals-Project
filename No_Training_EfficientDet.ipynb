{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance before training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# %pip install ensemble_boxes\n",
    "# %pip install albumentations\n",
    "# %pip install effdet\n",
    "# %pip install natsort\n",
    "#%pip install nibabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nigel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import cv2\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from effdet.efficientdet import HeadNet\n",
    "from albumentations import Compose, Resize, Normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.1+cu118\n",
      "CUDA available: True\n",
      "Number of CUDA devices: 1\n",
      "CUDA device name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Number of CUDA devices:\", torch.cuda.device_count())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print all the cases available in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cohort1': ['sub-101',\n",
       "  'sub-102',\n",
       "  'sub-103',\n",
       "  'sub-104',\n",
       "  'sub-105',\n",
       "  'sub-106',\n",
       "  'sub-107',\n",
       "  'sub-108',\n",
       "  'sub-109',\n",
       "  'sub-110',\n",
       "  'sub-111'],\n",
       " 'cohort2': ['sub-201',\n",
       "  'sub-202',\n",
       "  'sub-203',\n",
       "  'sub-204',\n",
       "  'sub-205',\n",
       "  'sub-206',\n",
       "  'sub-207',\n",
       "  'sub-208',\n",
       "  'sub-209',\n",
       "  'sub-210',\n",
       "  'sub-211',\n",
       "  'sub-212',\n",
       "  'sub-213',\n",
       "  'sub-214',\n",
       "  'sub-215',\n",
       "  'sub-216',\n",
       "  'sub-217',\n",
       "  'sub-218',\n",
       "  'sub-219',\n",
       "  'sub-220',\n",
       "  'sub-221',\n",
       "  'sub-222',\n",
       "  'sub-223',\n",
       "  'sub-224',\n",
       "  'sub-225',\n",
       "  'sub-226',\n",
       "  'sub-227',\n",
       "  'sub-228',\n",
       "  'sub-229',\n",
       "  'sub-230',\n",
       "  'sub-231',\n",
       "  'sub-232',\n",
       "  'sub-233',\n",
       "  'sub-234'],\n",
       " 'cohort3': ['sub-301',\n",
       "  'sub-302',\n",
       "  'sub-303',\n",
       "  'sub-304',\n",
       "  'sub-305',\n",
       "  'sub-306',\n",
       "  'sub-307',\n",
       "  'sub-308',\n",
       "  'sub-309',\n",
       "  'sub-310',\n",
       "  'sub-311',\n",
       "  'sub-312',\n",
       "  'sub-313',\n",
       "  'sub-314',\n",
       "  'sub-315',\n",
       "  'sub-316',\n",
       "  'sub-317',\n",
       "  'sub-318',\n",
       "  'sub-319',\n",
       "  'sub-320',\n",
       "  'sub-321',\n",
       "  'sub-322',\n",
       "  'sub-323',\n",
       "  'sub-324',\n",
       "  'sub-325',\n",
       "  'sub-326',\n",
       "  'sub-327']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "testing_label_relative = 'VALDO_Dataset\\Task2'\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "two_directories_up = os.path.abspath(os.path.join(current_directory, \"../\"))\n",
    "\n",
    "# Combine the current directory with the relative path\n",
    "testing_label_absolute = os.path.join(two_directories_up, testing_label_relative)\n",
    "\n",
    "folders = [item for item in os.listdir(testing_label_absolute) if os.path.isdir(os.path.join(testing_label_absolute, item))]\n",
    "\n",
    "cases = {\"cohort1\": [], \"cohort2\": [], \"cohort3\": []}\n",
    "# Print the list of folders\n",
    "for folder in folders:\n",
    "    if \"sub-1\" in folder:\n",
    "        cases[\"cohort1\"].append(folder)\n",
    "    elif \"sub-2\" in folder:\n",
    "        cases[\"cohort2\"].append(folder)\n",
    "    else:\n",
    "        cases[\"cohort3\"].append(folder)\n",
    "\n",
    "cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide the available cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nigel\\Documents\\VALDO_Dataset\\Task2\\sub-101\\sub-101_space-T2S_CMB.nii.gz\n",
      "c:\\Users\\nigel\\Documents\\VALDO_Dataset\\Task2\\sub-101\\sub-101_space-T2S_desc-masked_T2S.nii.gz\n"
     ]
    }
   ],
   "source": [
    "cohort1_labels = []\n",
    "cohort1_ids = []\n",
    "for case in cases[\"cohort1\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort1_labels.append(label)\n",
    "    cohort1_ids.append(id)\n",
    "# print(\"Label:\", cohort1_labels, cohort1_labels.__len__())\n",
    "# print(\"Ids:\", cohort1_ids, cohort1_ids.__len__())\n",
    "\n",
    "cohort2_labels = []\n",
    "cohort2_ids = []\n",
    "for case in cases[\"cohort2\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort2_labels.append(label)\n",
    "    cohort2_ids.append(id)\n",
    "# print(\"Label:\", cohort2_labels, cohort2_labels.__len__())\n",
    "# print(\"Ids:\", cohort2_ids, cohort2_ids.__len__())\n",
    "\n",
    "cohort3_labels = []\n",
    "cohort3_ids = []\n",
    "for case in cases[\"cohort3\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort3_labels.append(label)\n",
    "    cohort3_ids.append(id)\n",
    "# print(\"Label:\", cohort3_labels, cohort3_labels.__len__())\n",
    "# print(\"Ids:\", cohort3_ids, cohort3_ids.__len__())\n",
    "\n",
    "all_labels = cohort1_labels + cohort2_labels + cohort3_labels\n",
    "all_ids = cohort1_ids + cohort2_ids + cohort3_ids\n",
    "\n",
    "\n",
    "\n",
    "print(all_labels[0])\n",
    "print(all_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate for each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    slices = []\n",
    "    targets = []\n",
    "    img_paths = []\n",
    "    \n",
    "    for item in batch:\n",
    "        item_slices, item_targets, item_img_path = item\n",
    "        slices.extend(item_slices)\n",
    "        targets.extend(item_targets)\n",
    "        img_paths.append(item_img_path)\n",
    "\n",
    "    slices = [torch.stack(tuple(slice_set)) for slice_set in slices]\n",
    "    \n",
    "    return slices, targets, img_paths\n",
    "    \n",
    "def euclid_dist(t1, t2):\n",
    "    return np.sqrt(((t1-t2)**2).sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for VALDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VALDODataset(Dataset):\n",
    "    def __init__(self, img_paths, ann_paths, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.ann_paths = ann_paths\n",
    "        self.transform = transform\n",
    "\n",
    "        assert len(self.img_paths) == len(self.ann_paths), \"Mismatch between number of images and annotations\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = self.img_paths[idx]\n",
    "            ann_path = self.ann_paths[idx]\n",
    "\n",
    "            # Load 3D image\n",
    "            img = nib.load(img_path).get_fdata()\n",
    "            img = (img / np.max(img) * 255).astype(np.uint8)\n",
    "            \n",
    "            # Load 3D annotation\n",
    "            ann = nib.load(ann_path).get_fdata()\n",
    "            ann = (ann > 0).astype(np.uint8)  # Ensure mask is binary\n",
    "\n",
    "            slices = []\n",
    "            targets = []\n",
    "\n",
    "            for i in range(img.shape[2]):\n",
    "                img_slice = img[:, :, i]\n",
    "                ann_slice = ann[:, :, i]\n",
    "\n",
    "                img_slice = cv2.merge([img_slice] * 3)  # Convert single-channel to three-channel\n",
    "                boxes = self.extract_bounding_boxes(ann_slice)\n",
    "\n",
    "                if boxes:\n",
    "                    augmented = self.transform(image=img_slice, bboxes=boxes, labels=[1]*len(boxes))\n",
    "                    img_slice = augmented['image']\n",
    "                    boxes = augmented['bboxes']\n",
    "                    labels = augmented['labels']\n",
    "                else:\n",
    "                    augmented = self.transform(image=img_slice, bboxes=[], labels=[])\n",
    "                    img_slice = augmented['image']\n",
    "                    boxes = augmented['bboxes']\n",
    "                    labels = augmented['labels']\n",
    "\n",
    "                target = {\n",
    "                    'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
    "                    'labels': torch.tensor(labels, dtype=torch.int64)\n",
    "                }\n",
    "\n",
    "                slices.append(img_slice)\n",
    "                targets.append(target)\n",
    "\n",
    "            return slices, targets, img_path\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_bounding_boxes(self, mask):\n",
    "        # Extract bounding boxes from mask\n",
    "        boxes = []\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            # boxes.append([x, y, x + w, y + h])\n",
    "            boxes.append([x, y, x + 20, y + 20])\n",
    "        return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "        [\n",
    "            A.Resize(height=256, width=256, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    ")\n",
    "\n",
    "dataset = VALDODataset(img_paths=all_ids, ann_paths=all_labels, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 #any constant\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change into the device name of your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_num = torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear Exisisting GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "{'name': 'tf_efficientdet_d7', 'backbone_name': 'tf_efficientnet_b6', 'backbone_args': {'drop_path_rate': 0.2}, 'backbone_indices': None, 'image_size': [256, 256], 'num_classes': 1, 'min_level': 3, 'max_level': 7, 'num_levels': 5, 'num_scales': 3, 'aspect_ratios': [[1.0, 1.0], [1.4, 0.7], [0.7, 1.4]], 'anchor_scale': 5.0, 'pad_type': 'same', 'act_type': 'swish', 'norm_layer': None, 'norm_kwargs': {'eps': 0.001, 'momentum': 0.01}, 'box_class_repeats': 5, 'fpn_cell_repeats': 8, 'fpn_channels': 384, 'separable_conv': True, 'apply_resample_bn': True, 'conv_bn_relu_pattern': False, 'downsample_type': 'max', 'upsample_type': 'nearest', 'redundant_bias': True, 'head_bn_level_first': False, 'head_act_type': None, 'fpn_name': 'bifpn_sum', 'fpn_config': None, 'fpn_drop_path_rate': 0.0, 'alpha': 0.25, 'gamma': 1.5, 'label_smoothing': 0.0, 'legacy_focal': False, 'jit_loss': False, 'delta': 0.1, 'box_loss_weight': 50.0, 'soft_nms': False, 'max_detection_points': 5000, 'max_det_per_image': 100, 'url': 'https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d7-f05bf714.pth'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 prediction done\n",
      "Batch 1 prediction done\n",
      "Batch 2 prediction done\n",
      "Batch 3 prediction done\n",
      "Batch 4 prediction done\n",
      "Batch 5 prediction done\n",
      "Batch 6 prediction done\n",
      "Batch 7 prediction done\n",
      "Batch 8 prediction done\n",
      "Batch 9 prediction done\n",
      "Batch 10 prediction done\n",
      "Batch 11 prediction done\n",
      "Batch 12 prediction done\n",
      "Batch 13 prediction done\n",
      "Batch 14 prediction done\n",
      "Batch 15 prediction done\n",
      "Batch 16 prediction done\n",
      "Batch 17 prediction done\n",
      "Batch 18 prediction done\n",
      "Batch 19 prediction done\n",
      "Batch 20 prediction done\n",
      "Batch 21 prediction done\n",
      "Batch 22 prediction done\n",
      "Batch 23 prediction done\n",
      "Batch 24 prediction done\n",
      "Batch 25 prediction done\n",
      "Batch 26 prediction done\n",
      "Batch 27 prediction done\n",
      "Batch 28 prediction done\n",
      "Batch 29 prediction done\n",
      "Batch 30 prediction done\n",
      "Batch 31 prediction done\n",
      "Batch 32 prediction done\n",
      "Batch 33 prediction done\n",
      "Batch 34 prediction done\n",
      "Batch 35 prediction done\n",
      "Batch 36 prediction done\n",
      "Batch 37 prediction done\n",
      "Batch 38 prediction done\n",
      "Batch 39 prediction done\n",
      "Batch 40 prediction done\n",
      "Batch 41 prediction done\n",
      "Batch 42 prediction done\n",
      "Batch 43 prediction done\n",
      "Batch 44 prediction done\n",
      "Batch 45 prediction done\n",
      "Batch 46 prediction done\n",
      "Batch 47 prediction done\n",
      "Batch 48 prediction done\n",
      "Batch 49 prediction done\n",
      "Batch 50 prediction done\n",
      "Batch 51 prediction done\n",
      "Batch 52 prediction done\n",
      "Batch 53 prediction done\n",
      "Batch 54 prediction done\n",
      "Batch 55 prediction done\n",
      "Batch 56 prediction done\n",
      "Batch 57 prediction done\n",
      "Batch 58 prediction done\n",
      "Batch 59 prediction done\n",
      "Batch 60 prediction done\n",
      "Batch 61 prediction done\n",
      "Batch 62 prediction done\n",
      "Batch 63 prediction done\n",
      "Batch 64 prediction done\n",
      "Batch 65 prediction done\n",
      "Batch 66 prediction done\n",
      "Batch 67 prediction done\n",
      "Batch 68 prediction done\n",
      "Batch 69 prediction done\n",
      "Batch 70 prediction done\n",
      "Batch 71 prediction done\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 233471 KiB |   5947 MiB |   5719 GiB |   5719 GiB |\n",
      "|       from large pool | 133614 KiB |   5849 MiB |   5706 GiB |   5706 GiB |\n",
      "|       from small pool |  99857 KiB |    106 MiB |     12 GiB |     12 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 233471 KiB |   5947 MiB |   5719 GiB |   5719 GiB |\n",
      "|       from large pool | 133614 KiB |   5849 MiB |   5706 GiB |   5706 GiB |\n",
      "|       from small pool |  99857 KiB |    106 MiB |     12 GiB |     12 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 232875 KiB |   5946 MiB |   5713 GiB |   5712 GiB |\n",
      "|       from large pool | 133332 KiB |   5849 MiB |   5700 GiB |   5700 GiB |\n",
      "|       from small pool |  99543 KiB |    106 MiB |     12 GiB |     12 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 376832 KiB |  10012 MiB | 453724 MiB | 453356 MiB |\n",
      "|       from large pool | 270336 KiB |   9904 MiB | 453188 MiB | 452924 MiB |\n",
      "|       from small pool | 106496 KiB |    114 MiB |    536 MiB |    432 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 143360 KiB |   2413 MiB |   5411 GiB |   5410 GiB |\n",
      "|       from large pool | 136721 KiB |   2406 MiB |   5397 GiB |   5396 GiB |\n",
      "|       from small pool |   6639 KiB |      9 MiB |     13 GiB |     13 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1821    |    2024    |  304547    |  302726    |\n",
      "|       from large pool |      32    |      61    |   88342    |   88310    |\n",
      "|       from small pool |    1789    |    1987    |  216205    |  214416    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1821    |    2024    |  304547    |  302726    |\n",
      "|       from large pool |      32    |      61    |   88342    |   88310    |\n",
      "|       from small pool |    1789    |    1987    |  216205    |  214416    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      59    |      71    |    1181    |    1122    |\n",
      "|       from large pool |       7    |      16    |     913    |     906    |\n",
      "|       from small pool |      52    |      57    |     268    |     216    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      27    |      42    |  134993    |  134966    |\n",
      "|       from large pool |       6    |      17    |   43045    |   43039    |\n",
      "|       from small pool |      21    |      28    |   91948    |   91927    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading the model\n",
    "test_device = torch.device('cuda')\n",
    "print(test_device)\n",
    "\n",
    "config = get_efficientdet_config('tf_efficientdet_d7')\n",
    "config.update({'num_classes': 1})\n",
    "config.update({'image_size': (256, 256)})  # Adjust image size if needed\n",
    "config.update({\"url\": \"https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d7-f05bf714.pth\"})\n",
    "print(config)\n",
    "\n",
    "net = EfficientDet(config, pretrained_backbone=True)\n",
    "net.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "EffDet = DetBenchPredict(net)\n",
    "net.eval()\n",
    "device = torch.device(test_device)\n",
    "EffDet = EffDet.to(device)\n",
    "\n",
    "pre_test_dataset = VALDODataset(\n",
    "    img_paths=all_ids, ann_paths=all_labels, transform=transform\n",
    ")\n",
    "\n",
    "pre_test_dataloader_axial = DataLoader(\n",
    "    pre_test_dataset,\n",
    "    batch_size=1,  # Reduce batch size if needed\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "prediction_list = []\n",
    "for j, (images_axial, targets_axial, image_ids_axial) in enumerate(pre_test_dataloader_axial):\n",
    "    images_axial = torch.stack(images_axial).to(test_device).float()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        det = EffDet(images_axial)\n",
    "        for i in range(images_axial.shape[0]):\n",
    "            boxes = det[i].detach().cpu().numpy()[:, :4]\n",
    "            scores = det[i].detach().cpu().numpy()[:, 4]\n",
    "            indexes = np.where(scores > 0.1)[0]\n",
    "            boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "            preds.append({\n",
    "                'boxes': boxes,\n",
    "                'scores': scores,\n",
    "            })\n",
    "        prediction_list.append({\"predictions\": preds, \"id\": image_ids_axial})\n",
    "        print(f'Batch {j} prediction done')\n",
    "\n",
    "    # Clear cache after each batch\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "print(prediction_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put all the predicted boxes in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_boxes = []\n",
    "for i in range(len(prediction_list[0]['predictions'])):\n",
    "    # print(i, prediction_list[0]['predictions'][i])\n",
    "    predicted_boxes.append(prediction_list[0]['predictions'][i]['boxes'])\n",
    "predicted_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot each slice with the predicted boxes as green and true bloxes as blue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "# Assuming you have defined `targets` and `slices` elsewhere in your code\n",
    "slices, targets, id = dataset[0]\n",
    "# Calculate the number of subplots needed based on the length of your data\n",
    "num_slices = len(slices)\n",
    "num_cols = 5\n",
    "num_rows = (num_slices + num_cols - 1) // num_cols  # Round up to the nearest integer\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 3))\n",
    "print(targets[8]['boxes'])\n",
    "# Iterate over slices and targets\n",
    "for idx, (slice_base, target) in enumerate(zip(slices, targets)):\n",
    "    row = idx // num_cols\n",
    "    col = idx % num_cols\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Generate heatmap\n",
    "    heatmap_data = torch.mean(slice_base.float(), dim=0)\n",
    "    heatmap_data_np = heatmap_data.numpy()\n",
    "    sns.heatmap(heatmap_data_np, ax=ax)\n",
    "\n",
    "    # Generate bounding box\n",
    "    print(idx)\n",
    "    boxes = predicted_boxes[idx]\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        ax.add_patch(plt.Rectangle((x_min-8.5, y_min-8.5), x_max - x_min, y_max - y_min, \n",
    "                                    linewidth=2, edgecolor='g', facecolor='none'))\n",
    "    \n",
    "    boxes = target['boxes']\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        ax.add_patch(plt.Rectangle((x_min-8.5, y_min-8.5), x_max - x_min, y_max - y_min, \n",
    "                                    linewidth=2, edgecolor='b', facecolor='none'))\n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
