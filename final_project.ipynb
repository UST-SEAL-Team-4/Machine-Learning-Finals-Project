{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# %pip install ensemble_boxes\n",
    "# %pip install albumentations\n",
    "# %pip install effdet\n",
    "# %pip install natsort\n",
    "#%pip install nibabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nigel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from ensemble_boxes import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import natsort as ns\n",
    "import re\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n",
    "from effdet.efficientdet import HeadNet\n",
    "from torch.utils.data import Dataset\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For testing only\n",
    "from albumentations import Compose, Normalize, Resize, BboxParams\n",
    "from omegaconf import OmegaConf\n",
    "# from fitter import Fitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print all the cases available in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cohort1': ['sub-101',\n",
       "  'sub-102',\n",
       "  'sub-103',\n",
       "  'sub-104',\n",
       "  'sub-105',\n",
       "  'sub-106',\n",
       "  'sub-107',\n",
       "  'sub-108',\n",
       "  'sub-109',\n",
       "  'sub-110',\n",
       "  'sub-111'],\n",
       " 'cohort2': ['sub-201',\n",
       "  'sub-202',\n",
       "  'sub-203',\n",
       "  'sub-204',\n",
       "  'sub-205',\n",
       "  'sub-206',\n",
       "  'sub-207',\n",
       "  'sub-208',\n",
       "  'sub-209',\n",
       "  'sub-210',\n",
       "  'sub-211',\n",
       "  'sub-212',\n",
       "  'sub-213',\n",
       "  'sub-214',\n",
       "  'sub-215',\n",
       "  'sub-216',\n",
       "  'sub-217',\n",
       "  'sub-218',\n",
       "  'sub-219',\n",
       "  'sub-220',\n",
       "  'sub-221',\n",
       "  'sub-222',\n",
       "  'sub-223',\n",
       "  'sub-224',\n",
       "  'sub-225',\n",
       "  'sub-226',\n",
       "  'sub-227',\n",
       "  'sub-228',\n",
       "  'sub-229',\n",
       "  'sub-230',\n",
       "  'sub-231',\n",
       "  'sub-232',\n",
       "  'sub-233',\n",
       "  'sub-234'],\n",
       " 'cohort3': ['sub-301',\n",
       "  'sub-302',\n",
       "  'sub-303',\n",
       "  'sub-304',\n",
       "  'sub-305',\n",
       "  'sub-306',\n",
       "  'sub-307',\n",
       "  'sub-308',\n",
       "  'sub-309',\n",
       "  'sub-310',\n",
       "  'sub-311',\n",
       "  'sub-312',\n",
       "  'sub-313',\n",
       "  'sub-314',\n",
       "  'sub-315',\n",
       "  'sub-316',\n",
       "  'sub-317',\n",
       "  'sub-318',\n",
       "  'sub-319',\n",
       "  'sub-320',\n",
       "  'sub-321',\n",
       "  'sub-322',\n",
       "  'sub-323',\n",
       "  'sub-324',\n",
       "  'sub-325',\n",
       "  'sub-326',\n",
       "  'sub-327']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "testing_label_relative = 'VALDO_Dataset\\Task2'\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "two_directories_up = os.path.abspath(os.path.join(current_directory, \"../\"))\n",
    "\n",
    "# Combine the current directory with the relative path\n",
    "testing_label_absolute = os.path.join(two_directories_up, testing_label_relative)\n",
    "\n",
    "folders = [item for item in os.listdir(testing_label_absolute) if os.path.isdir(os.path.join(testing_label_absolute, item))]\n",
    "\n",
    "cases = {\"cohort1\": [], \"cohort2\": [], \"cohort3\": []}\n",
    "# Print the list of folders\n",
    "for folder in folders:\n",
    "    if \"sub-1\" in folder:\n",
    "        cases[\"cohort1\"].append(folder)\n",
    "    elif \"sub-2\" in folder:\n",
    "        cases[\"cohort2\"].append(folder)\n",
    "    else:\n",
    "        cases[\"cohort3\"].append(folder)\n",
    "\n",
    "cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the available cases according to cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nigel\\Documents\\VALDO_Dataset\\Task2\\sub-101\\sub-101_space-T2S_CMB.nii.gz\n",
      "c:\\Users\\nigel\\Documents\\VALDO_Dataset\\Task2\\sub-101\\sub-101_space-T2S_desc-masked_T2S.nii.gz\n"
     ]
    }
   ],
   "source": [
    "cohort1_labels = []\n",
    "cohort1_ids = []\n",
    "for case in cases[\"cohort1\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort1_labels.append(label)\n",
    "    cohort1_ids.append(id)\n",
    "# print(\"Label:\", cohort1_labels, cohort1_labels.__len__())\n",
    "# print(\"Ids:\", cohort1_ids, cohort1_ids.__len__())\n",
    "\n",
    "cohort2_labels = []\n",
    "cohort2_ids = []\n",
    "for case in cases[\"cohort2\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort2_labels.append(label)\n",
    "    cohort2_ids.append(id)\n",
    "# print(\"Label:\", cohort2_labels, cohort2_labels.__len__())\n",
    "# print(\"Ids:\", cohort2_ids, cohort2_ids.__len__())\n",
    "\n",
    "cohort3_labels = []\n",
    "cohort3_ids = []\n",
    "for case in cases[\"cohort3\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort3_labels.append(label)\n",
    "    cohort3_ids.append(id)\n",
    "# print(\"Label:\", cohort3_labels, cohort3_labels.__len__())\n",
    "# print(\"Ids:\", cohort3_ids, cohort3_ids.__len__())\n",
    "\n",
    "all_labels = cohort1_labels + cohort2_labels + cohort3_labels\n",
    "all_ids = cohort1_ids + cohort2_ids + cohort3_ids\n",
    "\n",
    "\n",
    "\n",
    "print(all_labels[0])\n",
    "print(all_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-101\\\\sub-101_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-102\\\\sub-102_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-103\\\\sub-103_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-104\\\\sub-104_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-105\\\\sub-105_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-106\\\\sub-106_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-107\\\\sub-107_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-108\\\\sub-108_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-109\\\\sub-109_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-110\\\\sub-110_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-111\\\\sub-111_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-201\\\\sub-201_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-202\\\\sub-202_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-203\\\\sub-203_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-204\\\\sub-204_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-205\\\\sub-205_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-206\\\\sub-206_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-207\\\\sub-207_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-208\\\\sub-208_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-209\\\\sub-209_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-210\\\\sub-210_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-211\\\\sub-211_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-212\\\\sub-212_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-213\\\\sub-213_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-214\\\\sub-214_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-215\\\\sub-215_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-216\\\\sub-216_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-217\\\\sub-217_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-218\\\\sub-218_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-219\\\\sub-219_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-220\\\\sub-220_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-221\\\\sub-221_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-222\\\\sub-222_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-223\\\\sub-223_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-224\\\\sub-224_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-225\\\\sub-225_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-226\\\\sub-226_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-227\\\\sub-227_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-228\\\\sub-228_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-229\\\\sub-229_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-230\\\\sub-230_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-231\\\\sub-231_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-232\\\\sub-232_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-233\\\\sub-233_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-234\\\\sub-234_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-301\\\\sub-301_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-302\\\\sub-302_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-303\\\\sub-303_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-304\\\\sub-304_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-305\\\\sub-305_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-306\\\\sub-306_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-307\\\\sub-307_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-308\\\\sub-308_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-309\\\\sub-309_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-310\\\\sub-310_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-311\\\\sub-311_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-312\\\\sub-312_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-313\\\\sub-313_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-314\\\\sub-314_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-315\\\\sub-315_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-316\\\\sub-316_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-317\\\\sub-317_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-318\\\\sub-318_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-319\\\\sub-319_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-320\\\\sub-320_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-321\\\\sub-321_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-322\\\\sub-322_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-323\\\\sub-323_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-324\\\\sub-324_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-325\\\\sub-325_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-326\\\\sub-326_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-327\\\\sub-327_space-T2S_CMB.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-101\\\\sub-101_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-102\\\\sub-102_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-103\\\\sub-103_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-104\\\\sub-104_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-105\\\\sub-105_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-106\\\\sub-106_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-107\\\\sub-107_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-108\\\\sub-108_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-109\\\\sub-109_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-110\\\\sub-110_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-111\\\\sub-111_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-201\\\\sub-201_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-202\\\\sub-202_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-203\\\\sub-203_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-204\\\\sub-204_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-205\\\\sub-205_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-206\\\\sub-206_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-207\\\\sub-207_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-208\\\\sub-208_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-209\\\\sub-209_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-210\\\\sub-210_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-211\\\\sub-211_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-212\\\\sub-212_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-213\\\\sub-213_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-214\\\\sub-214_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-215\\\\sub-215_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-216\\\\sub-216_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-217\\\\sub-217_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-218\\\\sub-218_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-219\\\\sub-219_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-220\\\\sub-220_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-221\\\\sub-221_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-222\\\\sub-222_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-223\\\\sub-223_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-224\\\\sub-224_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-225\\\\sub-225_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-226\\\\sub-226_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-227\\\\sub-227_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-228\\\\sub-228_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-229\\\\sub-229_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-230\\\\sub-230_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-231\\\\sub-231_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-232\\\\sub-232_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-233\\\\sub-233_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-234\\\\sub-234_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-301\\\\sub-301_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-302\\\\sub-302_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-303\\\\sub-303_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-304\\\\sub-304_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-305\\\\sub-305_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-306\\\\sub-306_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-307\\\\sub-307_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-308\\\\sub-308_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-309\\\\sub-309_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-310\\\\sub-310_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-311\\\\sub-311_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-312\\\\sub-312_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-313\\\\sub-313_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-314\\\\sub-314_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-315\\\\sub-315_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-316\\\\sub-316_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-317\\\\sub-317_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-318\\\\sub-318_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-319\\\\sub-319_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-320\\\\sub-320_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-321\\\\sub-321_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-322\\\\sub-322_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-323\\\\sub-323_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-324\\\\sub-324_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-325\\\\sub-325_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-326\\\\sub-326_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\nigel\\\\Documents\\\\VALDO_Dataset\\\\Task2\\\\sub-327\\\\sub-327_space-T2S_desc-masked_T2S.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "print(all_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset for VALDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VALDODataset(Dataset):\n",
    "    def __init__(self, img_paths, ann_paths, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.ann_paths = ann_paths\n",
    "        self.transform = transform\n",
    "\n",
    "        assert len(self.img_paths) == len(self.ann_paths), \"Mismatch between number of images and annotations\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = self.img_paths[idx]\n",
    "            ann_path = self.ann_paths[idx]\n",
    "\n",
    "            # Load 3D image\n",
    "            img = nib.load(img_path).get_fdata()\n",
    "            img = (img / np.max(img) * 255).astype(np.uint8)\n",
    "            \n",
    "            # Load 3D annotation\n",
    "            ann = nib.load(ann_path).get_fdata()\n",
    "            ann = (ann > 0).astype(np.uint8)  # Ensure mask is binary\n",
    "\n",
    "            slices = []\n",
    "            targets = []\n",
    "\n",
    "            for i in range(img.shape[2]):\n",
    "                img_slice = img[:, :, i]\n",
    "                ann_slice = ann[:, :, i]\n",
    "\n",
    "                img_slice = cv2.merge([img_slice] * 3)  # Convert single-channel to three-channel\n",
    "                boxes = self.extract_bounding_boxes(ann_slice)\n",
    "\n",
    "                if boxes:\n",
    "                    augmented = self.transform(image=img_slice, bboxes=boxes, labels=[1]*len(boxes))\n",
    "                    img_slice = augmented['image']\n",
    "                    boxes = augmented['bboxes']\n",
    "                    labels = augmented['labels']\n",
    "                else:\n",
    "                    augmented = self.transform(image=img_slice, bboxes=[], labels=[])\n",
    "                    img_slice = augmented['image']\n",
    "                    boxes = augmented['bboxes']\n",
    "                    labels = augmented['labels']\n",
    "\n",
    "                target = {\n",
    "                    'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
    "                    'labels': torch.tensor(labels, dtype=torch.int64)\n",
    "                }\n",
    "\n",
    "                slices.append(img_slice)\n",
    "                targets.append(target)\n",
    "\n",
    "            return slices, targets, img_path\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_bounding_boxes(self, mask):\n",
    "        # Extract bounding boxes from mask\n",
    "        boxes = []\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            # boxes.append([x, y, x + w, y + h])\n",
    "            boxes.append([x, y, x + 20, y + 20])\n",
    "        return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dataset creation and Data loader\n",
    "\n",
    "This also includes the transformation used in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "        [\n",
    "            A.Resize(height=256, width=256, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    ")\n",
    "\n",
    "dataset = VALDODataset(img_paths=all_ids, ann_paths=all_labels, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of slices: 35\n",
      "Total cases 72\n",
      "Shape of a slice: torch.Size([3, 256, 256]), target: {'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n"
     ]
    }
   ],
   "source": [
    "slices, targets, img_id = dataset[9]\n",
    "print(f\"Number of slices: {len(slices)}\")\n",
    "print('Total cases', dataset.__len__())\n",
    "print(f\"Shape of a slice: {slices[0].shape}, target: {targets[0]}\")\n",
    "\n",
    "# for i in targets:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.1+cu118\n",
      "CUDA available: True\n",
      "Number of CUDA devices: 1\n",
      "CUDA device name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Number of CUDA devices:\", torch.cuda.device_count())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 #any constant\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change into the device name of your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_num = torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the pretrained EfficientDet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_net(checkpoint_path):\n",
    "#     config = get_efficientdet_config('tf_efficientdet_d3')\n",
    "#     net = EfficientDet(config, pretrained_backbone=False)\n",
    "    \n",
    "#     config.num_classes = 1\n",
    "#     config.image_size=256\n",
    "#     net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "\n",
    "#     checkpoint = torch.load(checkpoint_path, map_location=device_num)\n",
    "#     net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#     del checkpoint\n",
    "#     gc.collect()\n",
    "\n",
    "#     net = DetBenchEval(net, config)\n",
    "#     net.eval()\n",
    "#     device = torch.device(device_num)\n",
    "#     return net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Axial marking from excel file\n",
    "\n",
    "from TPE-Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_axial_marking(label_path):\n",
    "    lists_dir = glob(label_path+'*') #label file directorie list\n",
    "    lists_dir.sort()\n",
    "\n",
    "    lists_name = [f for f in os.listdir(label_path) if not f.startswith('.')]   #label file list. Neglect hidden files\n",
    "    lists_name.sort()\n",
    "    lists_name\n",
    "\n",
    "    marking = pd.DataFrame(columns=['image_id', 'x', 'y', 'w', 'h'])\n",
    "\n",
    "    for i in range(len(lists_dir)):\n",
    "        xlsx = pd.read_excel(lists_dir[i], header = None)    \n",
    "        temp = pd.DataFrame(columns=['slice', 'x', 'y', 'class'])\n",
    "        temp2 = pd.DataFrame(columns=['image_id', 'x', 'y', 'w', 'h'])\n",
    "        for k in range(xlsx.shape[0]):\n",
    "            temp.loc[k] = list(xlsx.loc[k])\n",
    "        temp = temp.drop_duplicates(['x','y'], keep = 'first')       #drop out repeated 'x','y' values(= drop out same cmb) -\n",
    "        temp = temp.sort_values(by = 'slice',ignore_index=True)\n",
    "        for k in range(temp.shape[0]):\n",
    "            temp2.loc[k, 'image_id'] = lists_name[i].replace('.xlsx','')+ '_'+ str(temp.loc[k,'slice'])\n",
    "            temp2.loc[k, 'x'] = temp.loc[k,'x']-44    #Convert coordinates 256X448 -> 360X360\n",
    "            temp2.loc[k, 'y'] = temp.loc[k,'y']-76\n",
    "            temp2.loc[k, 'w'] = 20\n",
    "            temp2.loc[k, 'h'] = 20\n",
    "        marking = pd.concat([marking, temp2], ignore_index=True)\n",
    "    return marking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the table for 'whole' test set images\n",
    "\n",
    "from TPE-Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_whole_marking_axial(label_path,IMAGE_ROOT_PATH, marking_test):\n",
    "    lists_name = [f for f in os.listdir(label_path) if not f.startswith('.')]   #label file list. Neglect hidden files\n",
    "    lists_name.sort()\n",
    "    marking_test_all = pd.DataFrame(columns=['image_id', 'x', 'y', 'w', 'h'])\n",
    "\n",
    "    for i in range(len(lists_name)):\n",
    "\n",
    "        patient_name = lists_name[i].replace('.xlsx','')\n",
    "        im_list = [path.split('/')[-1][:-4] for path in glob(f'{IMAGE_ROOT_PATH}/{patient_name}_*.png')]\n",
    "        im_list = ns.natsorted(im_list)\n",
    "\n",
    "        temp2 = pd.DataFrame(columns=['image_id', 'x', 'y', 'w', 'h'])\n",
    "        temp2['image_id'] = im_list\n",
    "        temp2['x'] = 1\n",
    "        temp2['y'] = 1\n",
    "        temp2['w'] = 1\n",
    "        temp2['h'] = 1\n",
    "        marking_test_all = pd.concat([marking_test_all, temp2], ignore_index=True)\n",
    "\n",
    "    for i in range(len(marking_test)):     # fill the CMBs labels\n",
    "        index_num = marking_test_all.index[marking_test_all['image_id']==marking_test.loc[i,'image_id']].tolist()\n",
    "        if marking_test_all.loc[index_num[0],'x'] == 1:     #if it is first CMB on certain slice\n",
    "            marking_test_all.loc[index_num[0]] = marking_test.loc[i]\n",
    "        else:   #not first CMB on certain slice\n",
    "            temp1 = marking_test_all[marking_test_all.index < index_num[0]]\n",
    "            temp2 = marking_test_all[marking_test_all.index >= index_num[0]]\n",
    "            marking_test_all = temp1.append(marking_test.loc[i],ignore_index=True).append(temp2, ignore_index=True)\n",
    "    return marking_test_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizes the image and specifies the parameters for the bounding boxes\n",
    "\n",
    "from TPE-Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_transforms_axial():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=256, width=256, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset retriever from TPE-Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRetriever_cmbs:\n",
    "\n",
    "    def __init__(self, marking, image_ids, image_root_path, transforms=None, test=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = image_ids\n",
    "        self.marking = marking\n",
    "        self.transforms = transforms\n",
    "        self.test = test\n",
    "        self.image_root_path  = image_root_path\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "    \n",
    "        image, boxes = self.load_image_and_boxes(index)\n",
    "        \n",
    "        # there is only one class\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "\n",
    "\n",
    "        if self.transforms:\n",
    "            for i in range(10):\n",
    "                sample = self.transforms(**{\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels\n",
    "                })\n",
    "                if len(sample['bboxes']) > 0:       \n",
    "                    image = sample['image']\n",
    "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n",
    "                    break\n",
    "        else:\n",
    "            image = torch.tensor(image)\n",
    "            target['boxes'] = torch.tensor(boxes)\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "    def load_image_and_boxes(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = cv2.imread(f'{self.image_root_path}/{image_id}.png', cv2.IMREAD_UNCHANGED)    #get 16bit images\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)    #Convert BGR -> RGB\n",
    "        image/=65535.0\n",
    "        records = self.marking[self.marking['image_id'] == image_id]\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values \n",
    "        boxes[:, 0] = boxes[:, 0] - boxes[:, 2]/2         #transforms to left top corner&right bottom corner\n",
    "        boxes[:, 1] = boxes[:, 1] - boxes[:, 3]/2\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        return image, boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate for each batch\n",
    "\n",
    "This is used to return the slices, targets, and img_ids during each iteration in the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collate_fn(batch):\n",
    "#     return tuple(zip(*batch))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    slices = []\n",
    "    targets = []\n",
    "    img_paths = []\n",
    "    \n",
    "    for item in batch:\n",
    "        item_slices, item_targets, item_img_path = item\n",
    "        slices.extend(item_slices)\n",
    "        targets.extend(item_targets)\n",
    "        img_paths.append(item_img_path)\n",
    "\n",
    "    slices = [torch.stack(tuple(slice_set)) for slice_set in slices]\n",
    "    \n",
    "    return slices, targets, img_paths\n",
    "    \n",
    "def euclid_dist(t1, t2):\n",
    "    return np.sqrt(((t1-t2)**2).sum(axis = 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replaces occurrences of a substring from the right side of the original string with a new substring, up to a specified count.\n",
    "\n",
    "from TPE-Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceRight(original, old, new, count_right):\n",
    "    repeat=0\n",
    "    text = original\n",
    "    old_len = len(old)\n",
    "    \n",
    "    count_find = original.count(old)\n",
    "    if count_right > count_find: \n",
    "        repeat = count_find\n",
    "    else :\n",
    "        repeat = count_right\n",
    "\n",
    "    while(repeat):\n",
    "      find_index = text.rfind(old)\n",
    "      text = text[:find_index] + new + text[find_index+old_len:]\n",
    "\n",
    "      repeat -= 1\n",
    "      \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File paths for training and validation labels\n",
    "\n",
    "from TPE-Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label_path = '/data/labels/train/'\n",
    "# val_label_path = '/data/labels/validation/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the markings of the labels\n",
    "\n",
    "from TPE-Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marking_train = get_axial_marking(train_label_path)\n",
    "# marking_val = get_axial_marking(val_label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the dataset that will be used for the training\n",
    "\n",
    "from TPE-Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_aug = DatasetRetriever_cmbs(\n",
    "#     image_ids=np.array(marking_train['image_id']),  #array with image_ids\n",
    "#     marking=marking_train, \n",
    "#     transforms=get_train_transforms(),\n",
    "#     test=False,\n",
    "# )\n",
    "\n",
    "# validation_dataset = DatasetRetriever_cmbs(\n",
    "#     image_ids=np.array(marking_val['image_id']),\n",
    "#     marking=marking_val,\n",
    "#     transforms=get_valid_transforms(),\n",
    "#     test=True,\n",
    "# )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_ids, all_labels, test_size=0.30, random_state=40)\n",
    "\n",
    "train_dataset_aug = VALDODataset(\n",
    "    img_paths=X_train, ann_paths=y_train, transform=transform\n",
    ")\n",
    "\n",
    "validation_dataset = VALDODataset(\n",
    "    img_paths=X_test, ann_paths=y_test, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AverageMeter for the summary_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customized fitter class for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter:\n",
    "\n",
    "    def __init__(self, model, device, config):\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.base_dir = f'./{config.folder}'\n",
    "        if not os.path.exists(self.base_dir):\n",
    "            os.makedirs(self.base_dir)\n",
    "\n",
    "        self.log_path = f'{self.base_dir}/log.txt'\n",
    "        self.best_summary_loss = 10 ** 5\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "        self.log(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        summary_loss_over_itr_train = []\n",
    "        summary_loss_over_itr_val = []\n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss = self.train_one_epoch(train_loader)\n",
    "            summary_loss_over_itr_train.append(summary_loss)\n",
    "            self.log(\n",
    "                f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            self.save(f'{self.base_dir}/last-checkpoint.bin')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss = self.validation(validation_loader)\n",
    "            summary_loss_over_itr_val.append(summary_loss)\n",
    "            self.log(\n",
    "                f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            if summary_loss.avg < self.best_summary_loss:\n",
    "                self.best_summary_loss = summary_loss.avg\n",
    "                self.model.eval()\n",
    "                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
    "                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n",
    "                    os.remove(path)\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=summary_loss.avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "        return self.best_summary_loss, summary_loss_over_itr_train, summary_loss_over_itr_val\n",
    "\n",
    "\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        summary_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, targets, image_ids) in enumerate(val_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Val Step {step}/{len(val_loader)}, ' +\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, ' +\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                batch_size = len(images)\n",
    "                images = torch.stack(images).to(self.device).float()\n",
    "                boxes = [target['boxes'].to(self.device).float() for target in targets]\n",
    "                labels = [target['labels'].to(self.device).float() for target in targets]\n",
    "\n",
    "                for i in range(len(images)):\n",
    "                    img = images[i].unsqueeze(0)\n",
    "                    bbox = boxes[i]\n",
    "                    cls = labels[i]\n",
    "                    \n",
    "                    if bbox.nelement() == 0 or cls.nelement() == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    target = {\n",
    "                        \"bbox\": bbox.unsqueeze(0),\n",
    "                        \"cls\": cls.unsqueeze(0),\n",
    "                        'img_scale': None,\n",
    "                        'img_size': None,   \n",
    "                    }\n",
    "\n",
    "                    # loss, _, _, _ = self.model(img, target)\n",
    "                    output = self.model(img, target)\n",
    "                    # print(output, \"bobo\")\n",
    "                    summary_loss.update(output[\"loss\"].detach().item(), batch_size)\n",
    "\n",
    "        return summary_loss\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        summary_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        step_loss = []\n",
    "\n",
    "        for step, (images, targets, image_ids) in enumerate(train_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Train Step {step}/{len(train_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            batch_size = len(images[0])\n",
    "            images = [image.to(self.device).float() for image in images]\n",
    "            boxes = [target['boxes'].to(self.device).float() for target in targets]\n",
    "            labels = [target['labels'].to(self.device).float() for target in targets]\n",
    "            self.optimizer.zero_grad()\n",
    "                \n",
    "            for i in range(len(images)):\n",
    "                img = images[i].unsqueeze(0)\n",
    "                bbox = boxes[i]\n",
    "                cls = labels[i]\n",
    "\n",
    "                # Check if the current slice has any bounding boxes\n",
    "                if bbox.nelement() == 0 or cls.nelement() == 0:\n",
    "                    continue\n",
    "\n",
    "                target = {\n",
    "                    \"bbox\": boxes[i].unsqueeze(0),\n",
    "                    \"cls\": labels[i].unsqueeze(0)\n",
    "                }\n",
    "\n",
    "                output = self.model(img, target)\n",
    "\n",
    "                output['loss'].backward()\n",
    "                summary_loss.update(output['loss'].detach().item(), batch_size)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "\n",
    "        return summary_loss\n",
    "\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_summary_loss': self.best_summary_loss,\n",
    "            'epoch': self.epoch,\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
    "        self.epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "Adjusted the lr\n",
    "\n",
    "from TPE-Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    num_workers = 0\n",
    "    batch_size = 1\n",
    "    # n_epochs = 10\n",
    "    n_epochs = 1\n",
    "    # lr = 0.0001\n",
    "    lr = 0.01\n",
    "\n",
    "    folder = 'Model_Save(Axial)_D7'\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True\n",
    "    verbose_step = 1\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False # do scheduler.step after optimizer.step\n",
    "    epoch_scheduler = False\n",
    "    validation_scheduler = True # do scheduler.step after validation stage loss -> For scheduler 'ReduceLROnPlateau'\n",
    "    \n",
    "#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n",
    "#     scheduler_params = dict(\n",
    "#         max_lr=0.001,\n",
    "#         epochs=n_epochs,\n",
    "#         steps_per_epoch=2*int(len(train_dataset_aug) / batch_size),\n",
    "#         pct_start=0.31,\n",
    "#         anneal_strategy='cos', \n",
    "#         final_div_factor=10**4\n",
    "#     )\n",
    "\n",
    "#     SchedulerClass = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "#     scheduler_params = dict(\n",
    "#         T_0=5,        # Number of iterations for the first restart.\n",
    "#         T_mult=2,    \n",
    "#         eta_min=0.00004,\n",
    "#         last_epoch=-1, \n",
    "#         verbose=False\n",
    "#     )\n",
    "\n",
    "#     SchedulerClass = torch.optim.lr_scheduler.ExponentialLR\n",
    "#     scheduler_params = dict(\n",
    "#         gamma = 0.7\n",
    "#     )\n",
    "\n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='min',\n",
    "        factor=0.1,\n",
    "        patience=1,\n",
    "        verbose=False, \n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0, \n",
    "        min_lr=0,\n",
    "        eps=1e-08\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance before training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## Loading the model \n",
    "# test_device = torch.device('cuda')\n",
    "\n",
    "# print(test_device)\n",
    "# config = get_efficientdet_config('tf_efficientdet_d7')\n",
    "# config.update({'num_classes': 1})\n",
    "# config.update({'image_size': (256, 256)})\n",
    "# config.update({\"url\": \"https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d7-f05bf714.pth\"})\n",
    "\n",
    "# print(config)\n",
    "\n",
    "# net = EfficientDet(config, pretrained_backbone=False)\n",
    "# net.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# EffDet = DetBenchTrain(net, config)\n",
    "# net.eval()\n",
    "# device = torch.device(test_device)\n",
    "# EffDet = EffDet.to(device)\n",
    "\n",
    "\n",
    "# pre_test_dataset = VALDODataset(\n",
    "#     img_paths=all_ids, ann_paths=all_labels, transform=transform\n",
    "# )\n",
    "\n",
    "# pre_test_dataloader_axial = DataLoader(\n",
    "#     pre_test_dataset,\n",
    "#     batch_size=1,\n",
    "#     drop_last=False,   #drop last one for having same batch size\n",
    "#     num_workers=0,\n",
    "#     collate_fn=collate_fn,\n",
    "# )\n",
    "\n",
    "# prediction_list = []\n",
    "# for j, (images_axial, targets_axial, image_ids_axial) in enumerate(pre_test_dataloader_axial):\n",
    "#     images_axial = torch.stack(images_axial).to(test_device).float()\n",
    "#     preds = []\n",
    "#     det = EffDet(images_axial, targets_axial)\n",
    "#     for i in range(images_axial.shape[0]):\n",
    "#         boxes = det[i].detach().cpu().numpy()[:,:4]\n",
    "#         scores = det[i].detach().cpu().numpy()[:,4]\n",
    "#         indexes = np.where(scores > 0.1)[0]\n",
    "#         boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "#         boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "#         preds.append({\n",
    "#             'rois': boxes,\n",
    "#             'scores': scores,\n",
    "#         })\n",
    "#     prediction_list.append({\"predictions\": preds, \"id\": image_ids_axial})\n",
    "#     print(f'Batch {j} prediction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "{'name': 'tf_efficientdet_d7', 'backbone_name': 'tf_efficientnet_b6', 'backbone_args': {'drop_path_rate': 0.2}, 'backbone_indices': None, 'image_size': [256, 256], 'num_classes': 1, 'min_level': 3, 'max_level': 7, 'num_levels': 5, 'num_scales': 3, 'aspect_ratios': [[1.0, 1.0], [1.4, 0.7], [0.7, 1.4]], 'anchor_scale': 5.0, 'pad_type': 'same', 'act_type': 'swish', 'norm_layer': None, 'norm_kwargs': {'eps': 0.001, 'momentum': 0.01}, 'box_class_repeats': 5, 'fpn_cell_repeats': 8, 'fpn_channels': 384, 'separable_conv': True, 'apply_resample_bn': True, 'conv_bn_relu_pattern': False, 'downsample_type': 'max', 'upsample_type': 'nearest', 'redundant_bias': True, 'head_bn_level_first': False, 'head_act_type': None, 'fpn_name': 'bifpn_sum', 'fpn_config': None, 'fpn_drop_path_rate': 0.0, 'alpha': 0.25, 'gamma': 1.5, 'label_smoothing': 0.0, 'legacy_focal': False, 'jit_loss': False, 'delta': 0.1, 'box_loss_weight': 50.0, 'soft_nms': False, 'max_detection_points': 5000, 'max_det_per_image': 100, 'url': 'https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d7-f05bf714.pth'}\n",
      "Batch 0 prediction done\n",
      "Batch 1 prediction done\n",
      "Batch 2 prediction done\n",
      "Batch 3 prediction done\n",
      "Batch 4 prediction done\n",
      "Batch 5 prediction done\n",
      "Batch 6 prediction done\n",
      "Batch 7 prediction done\n",
      "Batch 8 prediction done\n",
      "Batch 9 prediction done\n",
      "Batch 10 prediction done\n",
      "Batch 11 prediction done\n",
      "Batch 12 prediction done\n",
      "Batch 13 prediction done\n",
      "Batch 14 prediction done\n",
      "Batch 15 prediction done\n",
      "Batch 16 prediction done\n",
      "Batch 17 prediction done\n",
      "Batch 18 prediction done\n",
      "Batch 19 prediction done\n",
      "Batch 20 prediction done\n",
      "Batch 21 prediction done\n",
      "Batch 22 prediction done\n",
      "Batch 23 prediction done\n",
      "Batch 24 prediction done\n",
      "Batch 25 prediction done\n",
      "Batch 26 prediction done\n",
      "Batch 27 prediction done\n",
      "Batch 28 prediction done\n",
      "Batch 29 prediction done\n",
      "Batch 30 prediction done\n",
      "Batch 31 prediction done\n",
      "Batch 32 prediction done\n",
      "Batch 33 prediction done\n",
      "Batch 34 prediction done\n",
      "Batch 35 prediction done\n",
      "Batch 36 prediction done\n",
      "Batch 37 prediction done\n",
      "Batch 38 prediction done\n",
      "Batch 39 prediction done\n",
      "Batch 40 prediction done\n",
      "Batch 41 prediction done\n",
      "Batch 42 prediction done\n",
      "Batch 43 prediction done\n",
      "Batch 44 prediction done\n",
      "Batch 45 prediction done\n",
      "Batch 46 prediction done\n",
      "Batch 47 prediction done\n",
      "Batch 48 prediction done\n",
      "Batch 49 prediction done\n",
      "Batch 50 prediction done\n",
      "Batch 51 prediction done\n",
      "Batch 52 prediction done\n",
      "Batch 53 prediction done\n",
      "Batch 54 prediction done\n",
      "Batch 55 prediction done\n",
      "Batch 56 prediction done\n",
      "Batch 57 prediction done\n",
      "Batch 58 prediction done\n",
      "Batch 59 prediction done\n",
      "Batch 60 prediction done\n",
      "Batch 61 prediction done\n",
      "Batch 62 prediction done\n",
      "Batch 63 prediction done\n",
      "Batch 64 prediction done\n",
      "Batch 65 prediction done\n",
      "Batch 66 prediction done\n",
      "Batch 67 prediction done\n",
      "Batch 68 prediction done\n",
      "Batch 69 prediction done\n",
      "Batch 70 prediction done\n",
      "Batch 71 prediction done\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 233471 KiB |   5947 MiB |   5693 GiB |   5693 GiB |\n",
      "|       from large pool | 133614 KiB |   5849 MiB |   5680 GiB |   5680 GiB |\n",
      "|       from small pool |  99857 KiB |    106 MiB |     13 GiB |     13 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 233471 KiB |   5947 MiB |   5693 GiB |   5693 GiB |\n",
      "|       from large pool | 133614 KiB |   5849 MiB |   5680 GiB |   5680 GiB |\n",
      "|       from small pool |  99857 KiB |    106 MiB |     13 GiB |     13 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 232875 KiB |   5946 MiB |   5687 GiB |   5687 GiB |\n",
      "|       from large pool | 133332 KiB |   5849 MiB |   5674 GiB |   5674 GiB |\n",
      "|       from small pool |  99543 KiB |    106 MiB |     13 GiB |     13 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 376832 KiB |  10012 MiB | 449948 MiB | 449580 MiB |\n",
      "|       from large pool | 270336 KiB |   9904 MiB | 449410 MiB | 449146 MiB |\n",
      "|       from small pool | 106496 KiB |    114 MiB |    538 MiB |    434 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 143360 KiB |   2413 MiB |   5452 GiB |   5452 GiB |\n",
      "|       from large pool | 136721 KiB |   2406 MiB |   5438 GiB |   5438 GiB |\n",
      "|       from small pool |   6639 KiB |      9 MiB |     14 GiB |     14 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1821    |    2024    |  304021    |  302200    |\n",
      "|       from large pool |      32    |      61    |   87607    |   87575    |\n",
      "|       from small pool |    1789    |    1987    |  216414    |  214625    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1821    |    2024    |  304021    |  302200    |\n",
      "|       from large pool |      32    |      61    |   87607    |   87575    |\n",
      "|       from small pool |    1789    |    1987    |  216414    |  214625    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      59    |      71    |    1154    |    1095    |\n",
      "|       from large pool |       7    |      16    |     885    |     878    |\n",
      "|       from small pool |      52    |      57    |     269    |     217    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      27    |      43    |  123069    |  123042    |\n",
      "|       from large pool |       6    |      17    |   42351    |   42345    |\n",
      "|       from small pool |      21    |      28    |   80718    |   80697    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear any existing GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Loading the model\n",
    "test_device = torch.device('cuda')\n",
    "print(test_device)\n",
    "\n",
    "config = get_efficientdet_config('tf_efficientdet_d7')\n",
    "config.update({'num_classes': 1})\n",
    "config.update({'image_size': (256, 256)})  # Adjust image size if needed\n",
    "config.update({\"url\": \"https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d7-f05bf714.pth\"})\n",
    "print(config)\n",
    "\n",
    "net = EfficientDet(config, pretrained_backbone=False)\n",
    "net.class_net = HeadNet(config, num_outputs=config.num_classes)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "EffDet = DetBenchPredict(net)\n",
    "net.eval()\n",
    "device = torch.device(test_device)\n",
    "EffDet = EffDet.to(device)\n",
    "\n",
    "pre_test_dataset = VALDODataset(\n",
    "    img_paths=all_ids, ann_paths=all_labels, transform=transform\n",
    ")\n",
    "\n",
    "pre_test_dataloader_axial = DataLoader(\n",
    "    pre_test_dataset,\n",
    "    batch_size=1,  # Reduce batch size if needed\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "prediction_list = []\n",
    "for j, (images_axial, targets_axial, image_ids_axial) in enumerate(pre_test_dataloader_axial):\n",
    "    images_axial = torch.stack(images_axial).to(test_device).float()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        det = EffDet(images_axial)\n",
    "        for i in range(images_axial.shape[0]):\n",
    "            boxes = det[i].detach().cpu().numpy()[:, :4]\n",
    "            scores = det[i].detach().cpu().numpy()[:, 4]\n",
    "            indexes = np.where(scores > 0.1)[0]\n",
    "            boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "            preds.append({\n",
    "                'boxes': boxes,\n",
    "                'scores': scores,\n",
    "            })\n",
    "        prediction_list.append({\"predictions\": preds, \"id\": image_ids_axial})\n",
    "        print(f'Batch {j} prediction done')\n",
    "\n",
    "    # Clear cache after each batch\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "print(prediction_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process of the model\n",
    "\n",
    "from TPE-Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "\n",
    "    net = get_net()\n",
    "    device = torch.device('cuda')\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset_aug,\n",
    "        batch_size=TrainGlobalConfig.batch_size,     \n",
    "        sampler=RandomSampler(train_dataset_aug),\n",
    "        pin_memory=False,\n",
    "        drop_last=False,   #drop last one for having same batch size\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, \n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        shuffle=False,\n",
    "        sampler=SequentialSampler(validation_dataset),\n",
    "        pin_memory=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "    best_val_loss, summary_loss_over_itr_train, summary_loss_over_itr_val = fitter.fit(train_loader, val_loader)\n",
    "    \n",
    "    return best_val_loss, summary_loss_over_itr_train, summary_loss_over_itr_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('efficientdet_d7-f05bf714.pth')\n",
    "pretrained_dict = {k: v for k, v in checkpoint.items() if 'class_net' not in k}\n",
    "config = get_efficientdet_config('tf_efficientdet_d7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d7_53-6d1d7a95.pth\n"
     ]
    }
   ],
   "source": [
    "# pretrained_dict.items()\n",
    "print(\"  \", config.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get the weights:\n",
    "\n",
    "1. Go to: https://github.com/rwightman/efficientdet-pytorch/releases\n",
    "2. Look for Weights in the bottom\n",
    "3. Find \"efficientdet_d7-f05bf714.pth\"\n",
    "4. Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will return the efficientdet model\n",
    "\n",
    "from TPE-Det but rearranged to accomodate tha changes in the APi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'tf_efficientdet_d7', 'backbone_name': 'tf_efficientnet_b6', 'backbone_args': {'drop_path_rate': 0.2}, 'backbone_indices': None, 'image_size': [256, 256], 'num_classes': 1, 'min_level': 3, 'max_level': 7, 'num_levels': 5, 'num_scales': 3, 'aspect_ratios': [[1.0, 1.0], [1.4, 0.7], [0.7, 1.4]], 'anchor_scale': 5.0, 'pad_type': 'same', 'act_type': 'swish', 'norm_layer': None, 'norm_kwargs': {'eps': 0.001, 'momentum': 0.01}, 'box_class_repeats': 5, 'fpn_cell_repeats': 8, 'fpn_channels': 384, 'separable_conv': True, 'apply_resample_bn': True, 'conv_bn_relu_pattern': False, 'downsample_type': 'max', 'upsample_type': 'nearest', 'redundant_bias': True, 'head_bn_level_first': False, 'head_act_type': None, 'fpn_name': 'bifpn_sum', 'fpn_config': None, 'fpn_drop_path_rate': 0.0, 'alpha': 0.25, 'gamma': 1.5, 'label_smoothing': 0.0, 'legacy_focal': False, 'jit_loss': False, 'delta': 0.1, 'box_loss_weight': 50.0, 'soft_nms': False, 'max_detection_points': 5000, 'max_det_per_image': 100, 'url': 'https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d7-f05bf714.pth'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:timm.models._builder:Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Fitter prepared. Device is cuda\n",
      "\n",
      "2024-05-18T09:49:03.751309\n",
      "LR: 0.01\n",
      "[RESULT]: Train. Epoch: 0, summary_loss: 65.91466, time: 97.48711\n",
      "[RESULT]: Val. Epoch: 0, summary_loss: 22323141.73635, time: 39.47926\n"
     ]
    }
   ],
   "source": [
    "def get_net():\n",
    "\n",
    "    config = get_efficientdet_config('tf_efficientdet_d7')\n",
    "    config.update({'num_classes': 1})\n",
    "    config.update({'image_size': (256, 256)})\n",
    "    config.update({\"url\": \"https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d7-f05bf714.pth\"})\n",
    "\n",
    "    print(config)\n",
    "\n",
    "    net = EfficientDet(config, pretrained_backbone=True)\n",
    "    # checkpoint = torch.load('efficientdet_d7-f05bf714.pth')\n",
    "    # net.load_state_dict(checkpoint, strict=False)\n",
    "\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes) #Use default batchnorm\n",
    "    \n",
    "    return DetBenchTrain(net, config)\n",
    "\n",
    "best_val_loss, summary_loss_over_itr_train, summary_loss_over_itr_val = run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_val_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbest_val_loss\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary_loss_over_itr_train)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(summary_loss_over_itr_val)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_val_loss' is not defined"
     ]
    }
   ],
   "source": [
    "print(best_val_loss)\n",
    "print(summary_loss_over_itr_train)\n",
    "print(summary_loss_over_itr_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Paths\n",
    "\n",
    "from TPE-Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_label_path = 'data/labels/test/'\n",
    "# IMAGE_ROOT_PATH_AXIAL = 'data/images/axial/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth annotations for CMB\n",
    "\n",
    "from TPE-Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_marking_cd_gt_axial(marking_test_axial):\n",
    "#     marking_cd_gt_axial = pd.DataFrame(columns=['patient_id', 's', 'x', 'y'])\n",
    "#     for i in range(len(marking_test_axial)):\n",
    "#         image_id = marking_test_axial.loc[i]['image_id']\n",
    "#         numbers_axial = re.findall(\"\\d+\", image_id)\n",
    "#         slice_num_axial = int(numbers_axial[-1])\n",
    "#         patient_id = replaceRight(image_id, '_'+str(slice_num_axial), '', 1)\n",
    "\n",
    "#         x = 256*marking_test_axial.loc[i]['x']/360\n",
    "#         y = 256*marking_test_axial.loc[i]['y']/360    \n",
    "\n",
    "#         temp = pd.DataFrame(columns=['patient_id', 's', 'x', 'y'])\n",
    "#         temp.loc[0] = [patient_id, slice_num_axial, x, y]\n",
    "#         marking_cd_gt_axial = pd.concat([marking_cd_gt_axial, temp], ignore_index=True)\n",
    "\n",
    "#     return marking_cd_gt_axial\n",
    "\n",
    "# num_cmbs=len(marking_test_all_axial[marking_test_all_axial['y']!=1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for returning the loaded network\n",
    "\n",
    "from TPE-Det but rearranged and changed the loaded version becuase there was a mismatch when I used the d3 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(checkpoint_path):\n",
    "    device = torch.device('cuda')\n",
    "    config = get_efficientdet_config('tf_efficientdet_d7') # Not sure if gagamitin dapat yung d7\n",
    "    \n",
    "    config.update({'num_classes': 1})\n",
    "    config.update({'image_size': (256, 256)})\n",
    "    config.update({\"norm_kwargs\": dict(eps=.001, momentum=.01)})\n",
    "\n",
    "\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes) # Configures the classification head of the model\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "\n",
    "    net = DetBenchPredict(net)\n",
    "    net.eval()\n",
    "    device = torch.device(device)\n",
    "    return net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the network\n",
    "\n",
    "You can get this in the Model_Save(Axial)_D7 directory\n",
    "\n",
    "This loads the best checkpoints in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_axial = load_net('Model_Save(Axial)_D7/best-checkpoint-008epoch.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction in axial\n",
    "\n",
    "from TPE-Det but modified the netaxial to not pass additional info of the images but can still be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_axial(images, score_threshold=0.1): #Confidence score...? Default 0.22\n",
    "    device = torch.device('cuda')\n",
    "    images = torch.stack(images).to(device).float()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        # det = net_axial(images, torch.tensor([1]*images.shape[0]).float().to(device))\n",
    "        det = net_axial(images)\n",
    "        for i in range(images.shape[0]):\n",
    "            boxes = det[i].detach().cpu().numpy()[:,:4]\n",
    "            scores = det[i].detach().cpu().numpy()[:,4]\n",
    "            indexes = np.where(scores > score_threshold)[0]\n",
    "            # print(indexes)\n",
    "            boxes = boxes[indexes]\n",
    "            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "            predictions.append({\n",
    "                'boxes': boxes[indexes],\n",
    "                'scores': scores[indexes],\n",
    "            })\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Boxes Fusion (WBF)\n",
    "\n",
    "from TPE-Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wbf_axial(predictions, image_index, image_size=256, iou_thr=0.45, skip_box_thr=0, weights=None):\n",
    "    boxes = [(prediction[image_index]['boxes']/(image_size-1)).tolist()  for prediction in predictions]\n",
    "    scores = [prediction[image_index]['scores'].tolist()  for prediction in predictions]\n",
    "    labels = [np.ones(prediction[image_index]['scores'].shape[0]).tolist() for prediction in predictions]\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting False Positives and True Positives\n",
    "\n",
    "from TPE-Det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_fptp(marking_cd_gt_patient, marking_cd_gt, marking_cd_slice, slice_num, patient_name, x, y):\n",
    "    #tp_candi -> near_gt: x,y cordi of ground truth from slices adjacent to predicting slice\n",
    "    #fp_candi -> near_pd: x,y cordi of predicted boxed from slices adjacent to predicting slice\n",
    "    #near_gt: ground truths (x, y cordi info) in near slices (upper and lower) from current images  \n",
    "    #near_pd: detections (slice, x, y cordi info) in near slices (only upper) from current images\n",
    "    global fp \n",
    "    global fp_count\n",
    "    global tp \n",
    "    global tp_count\n",
    "    \n",
    "    near_gt = marking_cd_gt_patient.loc[abs(slice_num-marking_cd_gt['s'])<5][['x','y']].to_numpy(dtype=float) #GT adjacent to prediction\n",
    "    near_pd = marking_cd_slice.loc[(slice_num-marking_cd_slice['s']<5)&(marking_cd_slice['patient_id']==patient_name)][['x','y']].to_numpy(dtype=float)\n",
    "\n",
    "    #count fp \n",
    "    if near_gt.shape[0]!=0:  #if predicted slice is adjacent to GT slices\n",
    "        dists_from_gt_list = euclid_dist([x, y], near_gt)\n",
    "        if dists_from_gt_list.min() > 20: #if the prediction is far from gt->FP in x, y cordi\n",
    "            #check whether the fp is consecutive or not\n",
    "            if near_pd.shape[0]!= 0: #\n",
    "                if euclid_dist([x, y], near_pd).min() > 10: #not consecutive detection.\n",
    "                    fp_count +=1\n",
    "                    fp = np.append(fp, np.array([[patient_name, slice_num, x, y, 0]]), axis=0)\n",
    "            else:\n",
    "                fp_count +=1\n",
    "                fp = np.append(fp, np.array([[patient_name, slice_num, x, y, 1]]), axis=0)\n",
    "        \n",
    "        else: #if the prediction is close to gt -> TP\n",
    "            for q in range(len(near_gt)):\n",
    "                near_gt_index = marking_cd_gt_patient.loc[(marking_cd_gt['x']==near_gt[q][0]) & (marking_cd_gt['y']==near_gt[q][1])].index[0]\n",
    "                if dists_from_gt_list[q] < 30 and marking_cd_gt_patient.loc[near_gt_index, 'state'] != 1: # if the gt is close to prediction and not detected.\n",
    "                    #check whether the tp is consecutive or not\n",
    "                    if near_pd.shape[0] != 0: #There are dets in the near slices \n",
    "                        if euclid_dist([x, y], near_pd).min() > 5: #there is no close x, y det -> not consecutive. 5\n",
    "                            tp_count += 1\n",
    "                            tp = np.append(tp, np.array([[patient_name, slice_num, x, y, 0]]), axis=0)\n",
    "                            marking_cd_gt_patient.loc[near_gt_index, 'state'] = 1\n",
    "                    else:\n",
    "                        tp_count += 1\n",
    "                        tp = np.append(tp, np.array([[patient_name, slice_num, x, y, 1]]), axis=0)\n",
    "                        marking_cd_gt_patient.loc[near_gt_index, 'state'] = 1\n",
    "                        \n",
    "    else:   #Predicted slice is not adjacent to that of GTs               \n",
    "        if i == 0:   #first slice\n",
    "            fp_count +=1\n",
    "            fp = np.append(fp, np.array([[patient_name, slice_num, x, y, 2]]), axis=0)\n",
    "        else:  \n",
    "            if near_pd.shape[0] != 0:\n",
    "                if euclid_dist([x, y], near_pd).min() > 10:\n",
    "                    fp_count +=1\n",
    "                    fp = np.append(fp, np.array([[patient_name, slice_num, x, y, 3]]), axis=0)\n",
    "            else:\n",
    "                fp_count +=1\n",
    "                fp = np.append(fp, np.array([[patient_name, slice_num, x, y, 4]]), axis=0)\n",
    "    return fp, fp_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset_all_axial = DatasetRetriever_cmbs(\n",
    "#     image_ids=np.array(marking_test_all_axial['image_id']),\n",
    "#     marking=marking_test_all_axial,\n",
    "#     transforms=get_valid_transforms_axial(),\n",
    "#     test=False,\n",
    "#     image_root_path = IMAGE_ROOT_PATH_AXIAL,\n",
    "# )\n",
    "\n",
    "# test_data_loader_all_axial = DataLoader(\n",
    "#     test_dataset_all_axial,\n",
    "#     batch_size=32,     #batchsize = 4\n",
    "#     drop_last=False,\n",
    "#     num_workers=0,\n",
    "#     collate_fn=collate_fn,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_all_axial = VALDODataset(\n",
    "    img_paths=cohort1_ids, ann_paths=cohort1_ids, transform=transform\n",
    ")\n",
    "\n",
    "test_data_loader_all_axial = DataLoader(\n",
    "    test_dataset_all_axial,\n",
    "    batch_size=1,\n",
    "    drop_last=False,   #drop last one for having same batch size\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = []\n",
    "for j, (images_axial, targets_axial, image_ids_axial) in enumerate(test_data_loader_all_axial):\n",
    "    predictions = make_predictions_axial(images_axial)\n",
    "    prediction_list.append({\"predictions\": predictions, \"id\": image_ids_axial})\n",
    "    print(f'Batch {j} prediction done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction_list[0]['id'])\n",
    "print(prediction_list[0]['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all the predicted boxes in a list\n",
    "\n",
    "Can also include the scores of each boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_boxes = []\n",
    "for i in range(len(prediction_list[0]['predictions'])):\n",
    "    # print(i, prediction_list[0]['predictions'][i])\n",
    "    predicted_boxes.append(prediction_list[0]['predictions'][i]['boxes'])\n",
    "predicted_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot each slice with the predicted boxes as green and true boxes as blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "# Assuming you have defined `targets` and `slices` elsewhere in your code\n",
    "slices, targets, id = dataset[0]\n",
    "# Calculate the number of subplots needed based on the length of your data\n",
    "num_slices = len(slices)\n",
    "num_cols = 5\n",
    "num_rows = (num_slices + num_cols - 1) // num_cols  # Round up to the nearest integer\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 3))\n",
    "print(targets[8]['boxes'])\n",
    "# Iterate over slices and targets\n",
    "for idx, (slice_base, target) in enumerate(zip(slices, targets)):\n",
    "    row = idx // num_cols\n",
    "    col = idx % num_cols\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    # Generate heatmap\n",
    "    heatmap_data = torch.mean(slice_base.float(), dim=0)\n",
    "    heatmap_data_np = heatmap_data.numpy()\n",
    "    sns.heatmap(heatmap_data_np, ax=ax)\n",
    "\n",
    "    # Generate bounding box\n",
    "    print(idx)\n",
    "    boxes = predicted_boxes[idx]\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        ax.add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "                                    linewidth=2, edgecolor='g', facecolor='none'))\n",
    "    \n",
    "    boxes = target['boxes']\n",
    "    for box in boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        ax.add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "                                    linewidth=2, edgecolor='b', facecolor='none'))\n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
