{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# %pip install ensemble_boxes\n",
    "# %pip install albumentations\n",
    "# %pip install effdet\n",
    "# %pip install natsort\n",
    "#%pip install nibabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from ensemble_boxes import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2\n",
    "import gc\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "import natsort as ns\n",
    "import re\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n",
    "from effdet.efficientdet import HeadNet\n",
    "from torch.utils.data import Dataset\n",
    "import nibabel as nib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For testing only\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations import Compose, Normalize, Resize, BboxParams\n",
    "from omegaconf import OmegaConf\n",
    "# from fitter import Fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\T'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\T'\n",
      "C:\\Users\\Lex Zedrick Lorenzo\\AppData\\Local\\Temp\\ipykernel_10372\\1612283650.py:3: SyntaxWarning: invalid escape sequence '\\T'\n",
      "  testing_label_relative = 'VALDO_Dataset\\Task2'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cohort1': ['sub-101',\n",
       "  'sub-102',\n",
       "  'sub-103',\n",
       "  'sub-104',\n",
       "  'sub-105',\n",
       "  'sub-106',\n",
       "  'sub-107',\n",
       "  'sub-108',\n",
       "  'sub-109',\n",
       "  'sub-110',\n",
       "  'sub-111'],\n",
       " 'cohort2': ['sub-201',\n",
       "  'sub-202',\n",
       "  'sub-203',\n",
       "  'sub-204',\n",
       "  'sub-205',\n",
       "  'sub-206',\n",
       "  'sub-207',\n",
       "  'sub-208',\n",
       "  'sub-209',\n",
       "  'sub-210',\n",
       "  'sub-211',\n",
       "  'sub-212',\n",
       "  'sub-213',\n",
       "  'sub-214',\n",
       "  'sub-215',\n",
       "  'sub-216',\n",
       "  'sub-217',\n",
       "  'sub-218',\n",
       "  'sub-219',\n",
       "  'sub-220',\n",
       "  'sub-221',\n",
       "  'sub-222',\n",
       "  'sub-223',\n",
       "  'sub-224',\n",
       "  'sub-225',\n",
       "  'sub-226',\n",
       "  'sub-227',\n",
       "  'sub-228',\n",
       "  'sub-229',\n",
       "  'sub-230',\n",
       "  'sub-231',\n",
       "  'sub-232',\n",
       "  'sub-233',\n",
       "  'sub-234'],\n",
       " 'cohort3': ['sub-301',\n",
       "  'sub-302',\n",
       "  'sub-303',\n",
       "  'sub-304',\n",
       "  'sub-305',\n",
       "  'sub-306',\n",
       "  'sub-307',\n",
       "  'sub-308',\n",
       "  'sub-309',\n",
       "  'sub-310',\n",
       "  'sub-311',\n",
       "  'sub-312',\n",
       "  'sub-313',\n",
       "  'sub-314',\n",
       "  'sub-315',\n",
       "  'sub-316',\n",
       "  'sub-317',\n",
       "  'sub-318',\n",
       "  'sub-319',\n",
       "  'sub-320',\n",
       "  'sub-321',\n",
       "  'sub-322',\n",
       "  'sub-323',\n",
       "  'sub-324',\n",
       "  'sub-325',\n",
       "  'sub-326',\n",
       "  'sub-327']}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "testing_label_relative = 'VALDO_Dataset\\Task2'\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "two_directories_up = os.path.abspath(os.path.join(current_directory, \"../\"))\n",
    "\n",
    "# Combine the current directory with the relative path\n",
    "testing_label_absolute = os.path.join(two_directories_up, testing_label_relative)\n",
    "\n",
    "folders = [item for item in os.listdir(testing_label_absolute) if os.path.isdir(os.path.join(testing_label_absolute, item))]\n",
    "\n",
    "cases = {\"cohort1\": [], \"cohort2\": [], \"cohort3\": []}\n",
    "# Print the list of folders\n",
    "for folder in folders:\n",
    "    if \"sub-1\" in folder:\n",
    "        cases[\"cohort1\"].append(folder)\n",
    "    elif \"sub-2\" in folder:\n",
    "        cases[\"cohort2\"].append(folder)\n",
    "    else:\n",
    "        cases[\"cohort3\"].append(folder)\n",
    "\n",
    "cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lex Zedrick Lorenzo\\Documents\\GitHub\\VALDO_Dataset\\Task2\\sub-101\\sub-101_space-T2S_CMB.nii.gz\n",
      "c:\\Users\\Lex Zedrick Lorenzo\\Documents\\GitHub\\VALDO_Dataset\\Task2\\sub-101\\sub-101_space-T2S_desc-masked_T2S.nii.gz\n"
     ]
    }
   ],
   "source": [
    "cohort1_labels = []\n",
    "cohort1_ids = []\n",
    "for case in cases[\"cohort1\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort1_labels.append(label)\n",
    "    cohort1_ids.append(id)\n",
    "# print(\"Label:\", cohort1_labels, cohort1_labels.__len__())\n",
    "# print(\"Ids:\", cohort1_ids, cohort1_ids.__len__())\n",
    "\n",
    "cohort2_labels = []\n",
    "cohort2_ids = []\n",
    "for case in cases[\"cohort2\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort2_labels.append(label)\n",
    "    cohort2_ids.append(id)\n",
    "# print(\"Label:\", cohort2_labels, cohort2_labels.__len__())\n",
    "# print(\"Ids:\", cohort2_ids, cohort2_ids.__len__())\n",
    "\n",
    "cohort3_labels = []\n",
    "cohort3_ids = []\n",
    "for case in cases[\"cohort3\"]:\n",
    "    label = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_CMB.nii.gz\"\n",
    "    id = f\"{testing_label_absolute}\\\\{case}\\\\{case}_space-T2S_desc-masked_T2S.nii.gz\"\n",
    "    cohort3_labels.append(label)\n",
    "    cohort3_ids.append(id)\n",
    "# print(\"Label:\", cohort3_labels, cohort3_labels.__len__())\n",
    "# print(\"Ids:\", cohort3_ids, cohort3_ids.__len__())\n",
    "\n",
    "all_labels = cohort1_labels + cohort2_labels + cohort3_labels\n",
    "all_ids = cohort1_ids + cohort2_ids + cohort3_ids\n",
    "\n",
    "\n",
    "\n",
    "print(all_labels[0])\n",
    "print(all_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-101\\\\sub-101_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-102\\\\sub-102_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-103\\\\sub-103_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-104\\\\sub-104_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-105\\\\sub-105_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-106\\\\sub-106_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-107\\\\sub-107_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-108\\\\sub-108_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-109\\\\sub-109_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-110\\\\sub-110_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-111\\\\sub-111_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-201\\\\sub-201_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-202\\\\sub-202_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-203\\\\sub-203_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-204\\\\sub-204_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-205\\\\sub-205_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-206\\\\sub-206_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-207\\\\sub-207_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-208\\\\sub-208_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-209\\\\sub-209_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-210\\\\sub-210_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-211\\\\sub-211_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-212\\\\sub-212_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-213\\\\sub-213_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-214\\\\sub-214_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-215\\\\sub-215_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-216\\\\sub-216_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-217\\\\sub-217_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-218\\\\sub-218_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-219\\\\sub-219_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-220\\\\sub-220_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-221\\\\sub-221_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-222\\\\sub-222_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-223\\\\sub-223_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-224\\\\sub-224_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-225\\\\sub-225_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-226\\\\sub-226_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-227\\\\sub-227_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-228\\\\sub-228_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-229\\\\sub-229_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-230\\\\sub-230_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-231\\\\sub-231_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-232\\\\sub-232_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-233\\\\sub-233_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-234\\\\sub-234_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-301\\\\sub-301_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-302\\\\sub-302_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-303\\\\sub-303_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-304\\\\sub-304_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-305\\\\sub-305_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-306\\\\sub-306_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-307\\\\sub-307_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-308\\\\sub-308_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-309\\\\sub-309_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-310\\\\sub-310_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-311\\\\sub-311_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-312\\\\sub-312_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-313\\\\sub-313_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-314\\\\sub-314_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-315\\\\sub-315_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-316\\\\sub-316_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-317\\\\sub-317_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-318\\\\sub-318_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-319\\\\sub-319_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-320\\\\sub-320_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-321\\\\sub-321_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-322\\\\sub-322_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-323\\\\sub-323_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-324\\\\sub-324_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-325\\\\sub-325_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-326\\\\sub-326_space-T2S_CMB.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-327\\\\sub-327_space-T2S_CMB.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-101\\\\sub-101_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-102\\\\sub-102_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-103\\\\sub-103_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-104\\\\sub-104_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-105\\\\sub-105_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-106\\\\sub-106_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-107\\\\sub-107_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-108\\\\sub-108_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-109\\\\sub-109_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-110\\\\sub-110_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-111\\\\sub-111_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-201\\\\sub-201_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-202\\\\sub-202_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-203\\\\sub-203_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-204\\\\sub-204_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-205\\\\sub-205_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-206\\\\sub-206_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-207\\\\sub-207_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-208\\\\sub-208_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-209\\\\sub-209_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-210\\\\sub-210_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-211\\\\sub-211_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-212\\\\sub-212_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-213\\\\sub-213_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-214\\\\sub-214_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-215\\\\sub-215_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-216\\\\sub-216_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-217\\\\sub-217_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-218\\\\sub-218_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-219\\\\sub-219_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-220\\\\sub-220_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-221\\\\sub-221_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-222\\\\sub-222_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-223\\\\sub-223_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-224\\\\sub-224_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-225\\\\sub-225_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-226\\\\sub-226_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-227\\\\sub-227_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-228\\\\sub-228_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-229\\\\sub-229_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-230\\\\sub-230_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-231\\\\sub-231_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-232\\\\sub-232_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-233\\\\sub-233_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-234\\\\sub-234_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-301\\\\sub-301_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-302\\\\sub-302_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-303\\\\sub-303_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-304\\\\sub-304_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-305\\\\sub-305_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-306\\\\sub-306_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-307\\\\sub-307_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-308\\\\sub-308_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-309\\\\sub-309_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-310\\\\sub-310_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-311\\\\sub-311_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-312\\\\sub-312_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-313\\\\sub-313_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-314\\\\sub-314_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-315\\\\sub-315_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-316\\\\sub-316_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-317\\\\sub-317_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-318\\\\sub-318_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-319\\\\sub-319_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-320\\\\sub-320_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-321\\\\sub-321_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-322\\\\sub-322_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-323\\\\sub-323_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-324\\\\sub-324_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-325\\\\sub-325_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-326\\\\sub-326_space-T2S_desc-masked_T2S.nii.gz', 'c:\\\\Users\\\\Lex Zedrick Lorenzo\\\\Documents\\\\GitHub\\\\VALDO_Dataset\\\\Task2\\\\sub-327\\\\sub-327_space-T2S_desc-masked_T2S.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "print(all_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset for VALDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VALDODataset(Dataset):\n",
    "    def __init__(self, img_paths, ann_paths, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.ann_paths = ann_paths\n",
    "        self.transform = transform\n",
    "\n",
    "        assert len(self.img_paths) == len(self.ann_paths), \"Mismatch between number of images and annotations\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        ann_path = self.ann_paths[idx]\n",
    "\n",
    "        # Load 3D image\n",
    "        img = nib.load(img_path).get_fdata()\n",
    "        img = (img / np.max(img) * 255).astype(np.uint8)\n",
    "        \n",
    "        # Load 3D annotation\n",
    "        ann = nib.load(ann_path).get_fdata()\n",
    "        ann = (ann > 0).astype(np.uint8)  # Ensure mask is binary\n",
    "\n",
    "        slices = []\n",
    "        targets = []\n",
    "\n",
    "        for i in range(img.shape[2]):\n",
    "            img_slice = img[:, :, i]\n",
    "            ann_slice = ann[:, :, i]\n",
    "\n",
    "            img_slice = cv2.merge([img_slice] * 3)  # Convert single-channel to three-channel\n",
    "            boxes = self.extract_bounding_boxes(ann_slice)\n",
    "\n",
    "            if boxes:\n",
    "                augmented = self.transform(image=img_slice, bboxes=boxes, labels=[1]*len(boxes))\n",
    "                img_slice = augmented['image']\n",
    "                boxes = augmented['bboxes']\n",
    "                labels = augmented['labels']\n",
    "            else:\n",
    "                augmented = self.transform(image=img_slice, bboxes=[], labels=[])\n",
    "                img_slice = augmented['image']\n",
    "                boxes = augmented['bboxes']\n",
    "                labels = augmented['labels']\n",
    "\n",
    "            target = {\n",
    "                'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
    "                'labels': torch.tensor(labels, dtype=torch.int64)\n",
    "            }\n",
    "\n",
    "            slices.append(img_slice)\n",
    "            targets.append(target)\n",
    "\n",
    "        return slices, targets\n",
    "\n",
    "    def extract_bounding_boxes(self, mask):\n",
    "        # Extract bounding boxes from mask\n",
    "        boxes = []\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            # boxes.append([x, y, x + w, y + h])\n",
    "            boxes.append([x, y, x + 20, y + 20])\n",
    "        return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "        [\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    ")\n",
    "\n",
    "dataset = VALDODataset(img_paths=all_ids, ann_paths=all_labels, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of slices: 35\n",
      "Total cases 72\n",
      "Shape of a slice: torch.Size([3, 512, 512]), target: {'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([[164., 327., 184., 347.],\n",
      "        [180., 299., 200., 319.]]), 'labels': tensor([1, 1])}\n",
      "{'boxes': tensor([[257., 320., 277., 340.]]), 'labels': tensor([1])}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([[278., 345., 298., 365.]]), 'labels': tensor([1])}\n",
      "{'boxes': tensor([[233., 321., 253., 341.],\n",
      "        [259., 313., 279., 333.]]), 'labels': tensor([1, 1])}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([[155., 188., 175., 208.]]), 'labels': tensor([1])}\n",
      "{'boxes': tensor([[267., 301., 287., 321.],\n",
      "        [264., 294., 284., 314.],\n",
      "        [271., 292., 291., 312.],\n",
      "        [268., 220., 288., 240.],\n",
      "        [259., 216., 279., 236.]]), 'labels': tensor([1, 1, 1, 1, 1])}\n",
      "{'boxes': tensor([[268., 288., 288., 308.],\n",
      "        [271., 226., 291., 246.]]), 'labels': tensor([1, 1])}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([[254., 303., 274., 323.]]), 'labels': tensor([1])}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "{'boxes': tensor([]), 'labels': tensor([], dtype=torch.int64)}\n",
      "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "slices, targets = dataset[0]\n",
    "print(f\"Number of slices: {len(slices)}\")\n",
    "print('Total cases', dataset.__len__())\n",
    "print(f\"Shape of a slice: {slices[0].shape}, target: {targets[0]}\")\n",
    "\n",
    "for i in targets:\n",
    "    print(i)\n",
    "\n",
    "print(slices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.0+cu118\n",
      "CUDA available: True\n",
      "Number of CUDA devices: 1\n",
      "CUDA device name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Number of CUDA devices:\", torch.cuda.device_count())\n",
    "    print(\"CUDA device name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42 #any constant\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change into the device name of your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_num = torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the pretrained EfficientDet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(checkpoint_path):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d3')\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    \n",
    "    config.num_classes = 1\n",
    "    config.image_size=512\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device_num)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "\n",
    "    net = DetBenchEval(net, config)\n",
    "    net.eval()\n",
    "    device = torch.device(device_num)\n",
    "    return net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Axial marking from excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_axial_marking(label_path):\n",
    "    lists_dir = glob(label_path+'*') #label file directorie list\n",
    "    lists_dir.sort()\n",
    "\n",
    "    lists_name = [f for f in os.listdir(label_path) if not f.startswith('.')]   #label file list. Neglect hidden files\n",
    "    lists_name.sort()\n",
    "    lists_name\n",
    "\n",
    "    marking = pd.DataFrame(columns=['image_id', 'x', 'y', 'w', 'h'])\n",
    "\n",
    "    for i in range(len(lists_dir)):\n",
    "        xlsx = pd.read_excel(lists_dir[i], header = None)    \n",
    "        temp = pd.DataFrame(columns=['slice', 'x', 'y', 'class'])\n",
    "        temp2 = pd.DataFrame(columns=['image_id', 'x', 'y', 'w', 'h'])\n",
    "        for k in range(xlsx.shape[0]):\n",
    "            temp.loc[k] = list(xlsx.loc[k])\n",
    "        temp = temp.drop_duplicates(['x','y'], keep = 'first')       #drop out repeated 'x','y' values(= drop out same cmb) -\n",
    "        temp = temp.sort_values(by = 'slice',ignore_index=True)\n",
    "        for k in range(temp.shape[0]):\n",
    "            temp2.loc[k, 'image_id'] = lists_name[i].replace('.xlsx','')+ '_'+ str(temp.loc[k,'slice'])\n",
    "            temp2.loc[k, 'x'] = temp.loc[k,'x']-44    #Convert coordinates 512X448 -> 360X360\n",
    "            temp2.loc[k, 'y'] = temp.loc[k,'y']-76\n",
    "            temp2.loc[k, 'w'] = 20\n",
    "            temp2.loc[k, 'h'] = 20\n",
    "        marking = pd.concat([marking, temp2], ignore_index=True)\n",
    "    return marking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the table for 'whole' test set images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_whole_marking_axial(label_path,IMAGE_ROOT_PATH, marking_test):\n",
    "    lists_name = [f for f in os.listdir(label_path) if not f.startswith('.')]   #label file list. Neglect hidden files\n",
    "    lists_name.sort()\n",
    "    marking_test_all = pd.DataFrame(columns=['image_id', 'x', 'y', 'w', 'h'])\n",
    "\n",
    "    for i in range(len(lists_name)):\n",
    "\n",
    "        patient_name = lists_name[i].replace('.xlsx','')\n",
    "        im_list = [path.split('/')[-1][:-4] for path in glob(f'{IMAGE_ROOT_PATH}/{patient_name}_*.png')]\n",
    "        im_list = ns.natsorted(im_list)\n",
    "\n",
    "        temp2 = pd.DataFrame(columns=['image_id', 'x', 'y', 'w', 'h'])\n",
    "        temp2['image_id'] = im_list\n",
    "        temp2['x'] = 1\n",
    "        temp2['y'] = 1\n",
    "        temp2['w'] = 1\n",
    "        temp2['h'] = 1\n",
    "        marking_test_all = pd.concat([marking_test_all, temp2], ignore_index=True)\n",
    "\n",
    "    for i in range(len(marking_test)):     # fill the CMBs labels\n",
    "        index_num = marking_test_all.index[marking_test_all['image_id']==marking_test.loc[i,'image_id']].tolist()\n",
    "        if marking_test_all.loc[index_num[0],'x'] == 1:     #if it is first CMB on certain slice\n",
    "            marking_test_all.loc[index_num[0]] = marking_test.loc[i]\n",
    "        else:   #not first CMB on certain slice\n",
    "            temp1 = marking_test_all[marking_test_all.index < index_num[0]]\n",
    "            temp2 = marking_test_all[marking_test_all.index >= index_num[0]]\n",
    "            marking_test_all = temp1.append(marking_test.loc[i],ignore_index=True).append(temp2, ignore_index=True)\n",
    "    return marking_test_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizes the image and specifies the parameters for the bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_transforms_axial():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRetriever_cmbs:\n",
    "\n",
    "    def __init__(self, marking, image_ids, image_root_path, transforms=None, test=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_ids = image_ids\n",
    "        self.marking = marking\n",
    "        self.transforms = transforms\n",
    "        self.test = test\n",
    "        self.image_root_path  = image_root_path\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        image_id = self.image_ids[index]\n",
    "    \n",
    "        image, boxes = self.load_image_and_boxes(index)\n",
    "        \n",
    "        # there is only one class\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.tensor([index])\n",
    "\n",
    "\n",
    "        if self.transforms:\n",
    "            for i in range(10):\n",
    "                sample = self.transforms(**{\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels\n",
    "                })\n",
    "                if len(sample['bboxes']) > 0:       \n",
    "                    image = sample['image']\n",
    "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  #yxyx: be warning\n",
    "                    break\n",
    "        else:\n",
    "            image = torch.tensor(image)\n",
    "            target['boxes'] = torch.tensor(boxes)\n",
    "\n",
    "        return image, target, image_id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.image_ids.shape[0]\n",
    "\n",
    "    def load_image_and_boxes(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "        image = cv2.imread(f'{self.image_root_path}/{image_id}.png', cv2.IMREAD_UNCHANGED)    #get 16bit images\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)    #Convert BGR -> RGB\n",
    "        image/=65535.0\n",
    "        records = self.marking[self.marking['image_id'] == image_id]\n",
    "        boxes = records[['x', 'y', 'w', 'h']].values \n",
    "        boxes[:, 0] = boxes[:, 0] - boxes[:, 2]/2         #transforms to left top corner&right bottom corner\n",
    "        boxes[:, 1] = boxes[:, 1] - boxes[:, 3]/2\n",
    "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "        return image, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "    \n",
    "def euclid_dist(t1, t2):\n",
    "    return np.sqrt(((t1-t2)**2).sum(axis = 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replaces occurrences of a substring from the right side of the original string with a new substring, up to a specified count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceRight(original, old, new, count_right):\n",
    "    repeat=0\n",
    "    text = original\n",
    "    old_len = len(old)\n",
    "    \n",
    "    count_find = original.count(old)\n",
    "    if count_right > count_find: \n",
    "        repeat = count_find\n",
    "    else :\n",
    "        repeat = count_right\n",
    "\n",
    "    while(repeat):\n",
    "      find_index = text.rfind(old)\n",
    "      text = text[:find_index] + new + text[find_index+old_len:]\n",
    "\n",
    "      repeat -= 1\n",
    "      \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File paths for training and validation labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label_path = '/data/labels/train/'\n",
    "# val_label_path = '/data/labels/validation/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the markings of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marking_train = get_axial_marking(train_label_path)\n",
    "# marking_val = get_axial_marking(val_label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the dataset that will be used for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_aug = DatasetRetriever_cmbs(\n",
    "#     image_ids=np.array(marking_train['image_id']),  #array with image_ids\n",
    "#     marking=marking_train, \n",
    "#     transforms=get_train_transforms(),\n",
    "#     test=False,\n",
    "# )\n",
    "\n",
    "# validation_dataset = DatasetRetriever_cmbs(\n",
    "#     image_ids=np.array(marking_val['image_id']),\n",
    "#     marking=marking_val,\n",
    "#     transforms=get_valid_transforms(),\n",
    "#     test=True,\n",
    "# )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    all_ids, all_labels, test_size=0.33, random_state=42)\n",
    "\n",
    "train_dataset_aug = VALDODataset(\n",
    "    img_paths=X_train, ann_paths=y_train, transform=transform\n",
    ")\n",
    "\n",
    "validation_dataset = VALDODataset(\n",
    "    img_paths=X_test, ann_paths=y_test, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fitter:\n",
    "\n",
    "    def __init__(self, model, device, config):\n",
    "        self.config = config\n",
    "        self.epoch = 0\n",
    "\n",
    "        self.base_dir = f'./{config.folder}'\n",
    "        if not os.path.exists(self.base_dir):\n",
    "            os.makedirs(self.base_dir)\n",
    "\n",
    "        self.log_path = f'{self.base_dir}/log.txt'\n",
    "        self.best_summary_loss = 10 ** 5\n",
    "\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.lr)\n",
    "        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n",
    "        self.log(f'Fitter prepared. Device is {self.device}')\n",
    "\n",
    "    def fit(self, train_loader, validation_loader):\n",
    "        for e in range(self.config.n_epochs):\n",
    "            if self.config.verbose:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                timestamp = datetime.utcnow().isoformat()\n",
    "                self.log(f'\\n{timestamp}\\nLR: {lr}')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss = self.train_one_epoch(train_loader)\n",
    "\n",
    "            self.log(\n",
    "                f'[RESULT]: Train. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            self.save(f'{self.base_dir}/last-checkpoint.bin')\n",
    "\n",
    "            t = time.time()\n",
    "            summary_loss = self.validation(validation_loader)\n",
    "\n",
    "            self.log(\n",
    "                f'[RESULT]: Val. Epoch: {self.epoch}, summary_loss: {summary_loss.avg:.5f}, time: {(time.time() - t):.5f}')\n",
    "            if summary_loss.avg < self.best_summary_loss:\n",
    "                self.best_summary_loss = summary_loss.avg\n",
    "                self.model.eval()\n",
    "                self.save(f'{self.base_dir}/best-checkpoint-{str(self.epoch).zfill(3)}epoch.bin')\n",
    "                for path in sorted(glob(f'{self.base_dir}/best-checkpoint-*epoch.bin'))[:-3]:\n",
    "                    os.remove(path)\n",
    "\n",
    "            if self.config.validation_scheduler:\n",
    "                self.scheduler.step(metrics=summary_loss.avg)\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def validation(self, val_loader):\n",
    "        self.model.eval()\n",
    "        summary_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, targets, image_ids) in enumerate(val_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Val Step {step}/{len(val_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            with torch.no_grad():\n",
    "                images = torch.stack(images)\n",
    "                batch_size = images.shape[0]\n",
    "                images = images.to(self.device).float()\n",
    "                boxes = [target['boxes'].to(self.device).float() for target in targets]\n",
    "                labels = [target['labels'].to(self.device).float() for target in targets]\n",
    "\n",
    "                loss, _, _ = self.model(images, boxes, labels)\n",
    "                summary_loss.update(loss.detach().item(), batch_size)\n",
    "\n",
    "        return summary_loss\n",
    "\n",
    "    def train_one_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        summary_loss = AverageMeter()\n",
    "        t = time.time()\n",
    "        for step, (images, targets, image_ids) in enumerate(train_loader):\n",
    "            if self.config.verbose:\n",
    "                if step % self.config.verbose_step == 0:\n",
    "                    print(\n",
    "                        f'Train Step {step}/{len(train_loader)}, ' + \\\n",
    "                        f'summary_loss: {summary_loss.avg:.5f}, ' + \\\n",
    "                        f'time: {(time.time() - t):.5f}', end='\\r'\n",
    "                    )\n",
    "            images = torch.stack(images)\n",
    "            images = images.to(self.device).float()\n",
    "            batch_size = images.shape[0]\n",
    "            boxes = [target['boxes'].to(self.device).float() for target in targets]\n",
    "            labels = [target['labels'].to(self.device).float() for target in targets]\n",
    "            self.optimizer.zero_grad()\n",
    "            loss, _, _ = self.model(images, boxes, labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            summary_loss.update(loss.detach().item(), batch_size)\n",
    "\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.config.step_scheduler:\n",
    "                self.scheduler.step()\n",
    "\n",
    "        return summary_loss\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.eval()\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_summary_loss': self.best_summary_loss,\n",
    "            'epoch': self.epoch,\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.best_summary_loss = checkpoint['best_summary_loss']\n",
    "        self.epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "    def log(self, message):\n",
    "        if self.config.verbose:\n",
    "            print(message)\n",
    "        with open(self.log_path, 'a+') as logger:\n",
    "            logger.write(f'{message}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainGlobalConfig:\n",
    "    num_workers = 1\n",
    "    batch_size = 1\n",
    "    n_epochs = 1\n",
    "    lr = 0.0001\n",
    "\n",
    "    folder = 'Model_Save(Axial)_D7'\n",
    "\n",
    "    # -------------------\n",
    "    verbose = True\n",
    "    verbose_step = 1\n",
    "    # -------------------\n",
    "\n",
    "    # --------------------\n",
    "    step_scheduler = False # do scheduler.step after optimizer.step\n",
    "    epoch_scheduler = False\n",
    "    validation_scheduler = True # do scheduler.step after validation stage loss -> For scheduler 'ReduceLROnPlateau'\n",
    "    \n",
    "#     SchedulerClass = torch.optim.lr_scheduler.OneCycleLR\n",
    "#     scheduler_params = dict(\n",
    "#         max_lr=0.001,\n",
    "#         epochs=n_epochs,\n",
    "#         steps_per_epoch=2*int(len(train_dataset_aug) / batch_size),\n",
    "#         pct_start=0.31,\n",
    "#         anneal_strategy='cos', \n",
    "#         final_div_factor=10**4\n",
    "#     )\n",
    "\n",
    "#     SchedulerClass = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts\n",
    "#     scheduler_params = dict(\n",
    "#         T_0=5,        # Number of iterations for the first restart.\n",
    "#         T_mult=2,    \n",
    "#         eta_min=0.00004,\n",
    "#         last_epoch=-1, \n",
    "#         verbose=False\n",
    "#     )\n",
    "\n",
    "#     SchedulerClass = torch.optim.lr_scheduler.ExponentialLR\n",
    "#     scheduler_params = dict(\n",
    "#         gamma = 0.7\n",
    "#     )\n",
    "\n",
    "    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "    scheduler_params = dict(\n",
    "        mode='min',\n",
    "        factor=0.1,\n",
    "        patience=1,\n",
    "        verbose=False, \n",
    "        threshold=0.0001,\n",
    "        threshold_mode='abs',\n",
    "        cooldown=0, \n",
    "        min_lr=0,\n",
    "        eps=1e-08\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "\n",
    "    net = get_net()\n",
    "    device = torch.device('cuda:0')\n",
    "    print('============================================================================================================================================================================')\n",
    "    print(device)\n",
    "    net.to(device)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset_aug,\n",
    "        batch_size=TrainGlobalConfig.batch_size,     \n",
    "        sampler=RandomSampler(train_dataset_aug),\n",
    "        pin_memory=False,\n",
    "        drop_last=False,   #drop last one for having same batch size\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        validation_dataset, \n",
    "        batch_size=TrainGlobalConfig.batch_size,\n",
    "        num_workers=TrainGlobalConfig.num_workers,\n",
    "        shuffle=False,\n",
    "        sampler=SequentialSampler(validation_dataset),\n",
    "        pin_memory=False,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "\n",
    "    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n",
    "    best_val_loss, summary_loss_over_itr_train, summary_loss_over_itr_val = fitter.fit(train_loader, val_loader)\n",
    "    \n",
    "    return best_val_loss, summary_loss_over_itr_train, summary_loss_over_itr_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will return the efficientdet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get the weights:\n",
    "\n",
    "1. Go to: https://github.com/rwightman/efficientdet-pytorch/releases\n",
    "2. Look for Weights in the bottom\n",
    "3. Find \"efficientdet_d7-f05bf714.pth\"\n",
    "4. Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_efficientdet_config('tf_efficientdet_d7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.image_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================================================\n",
      "cuda:0\n",
      "Fitter prepared. Device is cuda:0\n",
      "\n",
      "2024-05-15T11:34:19.328997\n",
      "LR: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lex Zedrick Lorenzo\\AppData\\Local\\Temp\\ipykernel_10372\\448305880.py:32: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = datetime.utcnow().isoformat()\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 27956) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Lex Zedrick Lorenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\Lex Zedrick Lorenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[155], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m     net\u001b[38;5;241m.\u001b[39mclass_net \u001b[38;5;241m=\u001b[39m HeadNet(mutable_config, num_outputs\u001b[38;5;241m=\u001b[39mmutable_config\u001b[38;5;241m.\u001b[39mnum_classes) \u001b[38;5;66;03m#Use default batchnorm\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DetBenchTrain(net, mutable_config)\n\u001b[1;32m---> 18\u001b[0m best_val_loss, summary_loss_over_itr_train, summary_loss_over_itr_val \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[150], line 29\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m     19\u001b[0m     validation_dataset, \n\u001b[0;32m     20\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mTrainGlobalConfig\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m fitter \u001b[38;5;241m=\u001b[39m Fitter(model\u001b[38;5;241m=\u001b[39mnet, device\u001b[38;5;241m=\u001b[39mdevice, config\u001b[38;5;241m=\u001b[39mTrainGlobalConfig)\n\u001b[1;32m---> 29\u001b[0m best_val_loss, summary_loss_over_itr_train, summary_loss_over_itr_val \u001b[38;5;241m=\u001b[39m \u001b[43mfitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_val_loss, summary_loss_over_itr_train, summary_loss_over_itr_val\n",
      "Cell \u001b[1;32mIn[147], line 36\u001b[0m, in \u001b[0;36mFitter.fit\u001b[1;34m(self, train_loader, validation_loader)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 36\u001b[0m summary_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[RESULT]: Train. Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, summary_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummary_loss\u001b[38;5;241m.\u001b[39mavg\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/last-checkpoint.bin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[147], line 87\u001b[0m, in \u001b[0;36mFitter.train_one_epoch\u001b[1;34m(self, train_loader)\u001b[0m\n\u001b[0;32m     85\u001b[0m summary_loss \u001b[38;5;241m=\u001b[39m AverageMeter()\n\u001b[0;32m     86\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 87\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose_step\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lex Zedrick Lorenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Lex Zedrick Lorenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lex Zedrick Lorenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Lex Zedrick Lorenzo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 27956) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "def get_net():\n",
    "    config = get_efficientdet_config('tf_efficientdet_d7')\n",
    "\n",
    "    \n",
    "\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    checkpoint = torch.load('efficientdet_d7-f05bf714.pth')\n",
    "    net.load_state_dict(checkpoint)\n",
    "\n",
    "    mutable_config = OmegaConf.create(OmegaConf.to_container(config, resolve=True))\n",
    "    mutable_config.num_classes = 1\n",
    "    mutable_config.image_size = [512, 512]\n",
    "\n",
    "    net.class_net = HeadNet(mutable_config, num_outputs=mutable_config.num_classes) #Use default batchnorm\n",
    "    \n",
    "    return DetBenchTrain(net, mutable_config)\n",
    "\n",
    "best_val_loss, summary_loss_over_itr_train, summary_loss_over_itr_val = run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_path = 'data/labels/test/'\n",
    "IMAGE_ROOT_PATH_AXIAL = 'data/images/axial/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth annotations for CMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Lex Zedrick Lorenzo\\AppData\\Local\\Temp\\ipykernel_16828\\2908703177.py:5: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  numbers_axial = re.findall(\"\\d+\", image_id)\n",
      "C:\\Users\\Lex Zedrick Lorenzo\\AppData\\Local\\Temp\\ipykernel_16828\\2908703177.py:5: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  numbers_axial = re.findall(\"\\d+\", image_id)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'marking_test_all_axial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m         marking_cd_gt_axial \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([marking_cd_gt_axial, temp], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m marking_cd_gt_axial\n\u001b[1;32m---> 18\u001b[0m num_cmbs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mmarking_test_all_axial\u001b[49m[marking_test_all_axial[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'marking_test_all_axial' is not defined"
     ]
    }
   ],
   "source": [
    "def make_marking_cd_gt_axial(marking_test_axial):\n",
    "    marking_cd_gt_axial = pd.DataFrame(columns=['patient_id', 's', 'x', 'y'])\n",
    "    for i in range(len(marking_test_axial)):\n",
    "        image_id = marking_test_axial.loc[i]['image_id']\n",
    "        numbers_axial = re.findall(\"\\d+\", image_id)\n",
    "        slice_num_axial = int(numbers_axial[-1])\n",
    "        patient_id = replaceRight(image_id, '_'+str(slice_num_axial), '', 1)\n",
    "\n",
    "        x = 512*marking_test_axial.loc[i]['x']/360\n",
    "        y = 512*marking_test_axial.loc[i]['y']/360    \n",
    "\n",
    "        temp = pd.DataFrame(columns=['patient_id', 's', 'x', 'y'])\n",
    "        temp.loc[0] = [patient_id, slice_num_axial, x, y]\n",
    "        marking_cd_gt_axial = pd.concat([marking_cd_gt_axial, temp], ignore_index=True)\n",
    "\n",
    "    return marking_cd_gt_axial\n",
    "\n",
    "num_cmbs=len(marking_test_all_axial[marking_test_all_axial['y']!=1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions in Axial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_axial(images, score_threshold=0.29): #Confidence score...? Default 0.22\n",
    "    device = torch.device(device_num)\n",
    "    images = torch.stack(images).to(device).float()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        det = net_axial(images, torch.tensor([1]*images.shape[0]).float().to(device))\n",
    "        for i in range(images.shape[0]):\n",
    "            boxes = det[i].detach().cpu().numpy()[:,:4]    \n",
    "            scores = det[i].detach().cpu().numpy()[:,4]\n",
    "            indexes = np.where(scores > score_threshold)[0]\n",
    "            boxes = boxes[indexes]\n",
    "            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "            predictions.append({\n",
    "                'boxes': boxes[indexes],\n",
    "                'scores': scores[indexes],\n",
    "            })\n",
    "    return [predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Boxes Fusion (WBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wbf_axial(predictions, image_index, image_size=512, iou_thr=0.45, skip_box_thr=0, weights=None):\n",
    "    boxes = [(prediction[image_index]['boxes']/(image_size-1)).tolist()  for prediction in predictions]\n",
    "    scores = [prediction[image_index]['scores'].tolist()  for prediction in predictions]\n",
    "    labels = [np.ones(prediction[image_index]['scores'].shape[0]).tolist() for prediction in predictions]\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting False Positives and True Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_fptp(marking_cd_gt_patient, marking_cd_gt, marking_cd_slice, slice_num, patient_name, x, y):\n",
    "    #tp_candi -> near_gt: x,y cordi of ground truth from slices adjacent to predicting slice\n",
    "    #fp_candi -> near_pd: x,y cordi of predicted boxed from slices adjacent to predicting slice\n",
    "    #near_gt: ground truths (x, y cordi info) in near slices (upper and lower) from current images  \n",
    "    #near_pd: detections (slice, x, y cordi info) in near slices (only upper) from current images\n",
    "    global fp \n",
    "    global fp_count\n",
    "    global tp \n",
    "    global tp_count\n",
    "    \n",
    "    near_gt = marking_cd_gt_patient.loc[abs(slice_num-marking_cd_gt['s'])<5][['x','y']].to_numpy(dtype=float) #GT adjacent to prediction\n",
    "    near_pd = marking_cd_slice.loc[(slice_num-marking_cd_slice['s']<5)&(marking_cd_slice['patient_id']==patient_name)][['x','y']].to_numpy(dtype=float)\n",
    "\n",
    "    #count fp \n",
    "    if near_gt.shape[0]!=0:  #if predicted slice is adjacent to GT slices\n",
    "        dists_from_gt_list = euclid_dist([x, y], near_gt)\n",
    "        if dists_from_gt_list.min() > 20: #if the prediction is far from gt->FP in x, y cordi\n",
    "            #check whether the fp is consecutive or not\n",
    "            if near_pd.shape[0]!= 0: #\n",
    "                if euclid_dist([x, y], near_pd).min() > 10: #not consecutive detection.\n",
    "                    fp_count +=1\n",
    "                    fp = np.append(fp, np.array([[patient_name, slice_num, x, y, 0]]), axis=0)\n",
    "            else:\n",
    "                fp_count +=1\n",
    "                fp = np.append(fp, np.array([[patient_name, slice_num, x, y, 1]]), axis=0)\n",
    "        \n",
    "        else: #if the prediction is close to gt -> TP\n",
    "            for q in range(len(near_gt)):\n",
    "                near_gt_index = marking_cd_gt_patient.loc[(marking_cd_gt['x']==near_gt[q][0]) & (marking_cd_gt['y']==near_gt[q][1])].index[0]\n",
    "                if dists_from_gt_list[q] < 30 and marking_cd_gt_patient.loc[near_gt_index, 'state'] != 1: # if the gt is close to prediction and not detected.\n",
    "                    #check whether the tp is consecutive or not\n",
    "                    if near_pd.shape[0] != 0: #There are dets in the near slices \n",
    "                        if euclid_dist([x, y], near_pd).min() > 5: #there is no close x, y det -> not consecutive. 5\n",
    "                            tp_count += 1\n",
    "                            tp = np.append(tp, np.array([[patient_name, slice_num, x, y, 0]]), axis=0)\n",
    "                            marking_cd_gt_patient.loc[near_gt_index, 'state'] = 1\n",
    "                    else:\n",
    "                        tp_count += 1\n",
    "                        tp = np.append(tp, np.array([[patient_name, slice_num, x, y, 1]]), axis=0)\n",
    "                        marking_cd_gt_patient.loc[near_gt_index, 'state'] = 1\n",
    "                        \n",
    "    else:   #Predicted slice is not adjacent to that of GTs               \n",
    "        if i == 0:   #first slice\n",
    "            fp_count +=1\n",
    "            fp = np.append(fp, np.array([[patient_name, slice_num, x, y, 2]]), axis=0)\n",
    "        else:  \n",
    "            if near_pd.shape[0] != 0:\n",
    "                if euclid_dist([x, y], near_pd).min() > 10:\n",
    "                    fp_count +=1\n",
    "                    fp = np.append(fp, np.array([[patient_name, slice_num, x, y, 3]]), axis=0)\n",
    "            else:\n",
    "                fp_count +=1\n",
    "                fp = np.append(fp, np.array([[patient_name, slice_num, x, y, 4]]), axis=0)\n",
    "    return fp, fp_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not Sure pero sabi ni ChatGPT pang evaluate daw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net(checkpoint_path):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d3')\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    \n",
    "    config.num_classes = 1\n",
    "    config.image_size=512\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01)) # Configures the classification head of the model\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device_num)\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    del checkpoint\n",
    "    gc.collect()\n",
    "\n",
    "    net = DetBenchEval(net, config)\n",
    "    net.eval();\n",
    "    device = torch.device(device_num)\n",
    "    return net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_axial = load_net('../Model/best-checkpoint.bin')\n",
    "net_sagittal = load_net('../Model/best-checkpoint.bin')\n",
    "net_coronal = load_net('../Model/best-checkpoint.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_all_axial = DatasetRetriever_cmbs(\n",
    "    image_ids=np.array(marking_test_all_axial['image_id']),\n",
    "    marking=marking_test_all_axial,\n",
    "    transforms=get_valid_transforms_axial(),\n",
    "    test=False,\n",
    "    image_root_path = IMAGE_ROOT_PATH_AXIAL,\n",
    ")\n",
    "\n",
    "test_data_loader_all_axial = DataLoader(\n",
    "    test_dataset_all_axial,\n",
    "    batch_size=32,     #batchsize = 4\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions_axial(images, score_threshold=0.29): #Confidence score...? Default 0.22\n",
    "    device = torch.device(device_num)\n",
    "    images = torch.stack(images).to(device).float()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        det = net_axial(images, torch.tensor([1]*images.shape[0]).float().to(device))\n",
    "        for i in range(images.shape[0]):\n",
    "            boxes = det[i].detach().cpu().numpy()[:,:4]    \n",
    "            scores = det[i].detach().cpu().numpy()[:,4]\n",
    "            indexes = np.where(scores > score_threshold)[0]\n",
    "            boxes = boxes[indexes]\n",
    "            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "            predictions.append({\n",
    "                'boxes': boxes[indexes],\n",
    "                'scores': scores[indexes],\n",
    "            })\n",
    "    return [predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_wbf_axial(predictions, image_index, image_size=512, iou_thr=0.45, skip_box_thr=0, weights=None):\n",
    "    boxes = [(prediction[image_index]['boxes']/(image_size-1)).tolist()  for prediction in predictions]\n",
    "    scores = [prediction[image_index]['scores'].tolist()  for prediction in predictions]\n",
    "    labels = [np.ones(prediction[image_index]['scores'].shape[0]).tolist() for prediction in predictions]\n",
    "    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n",
    "    boxes = boxes*(image_size-1)\n",
    "    return boxes, scores, labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
